{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Policy iteration and value iteration\n",
        "\n",
        "In this notebook, you will implement different dynamic programming approaches described in [Sutton and Barto's book, Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html). A grid ```World``` class and policy iteration has been implemented. Feel free to add more actions, rewards and/or terminals, or to modify the code to suit your needs."
      ],
      "metadata": {
        "id": "WMapQn0-y2Bk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install dependencies"
      ],
      "metadata": {
        "id": "e9vApPmQFGUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install numpy pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9pkLaPNMOTP",
        "outputId": "8adb5191-7368-4ed0-ecf0-730f2fe217ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "Q8kdEPNmFOCr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Wl4F2OZK3Xz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sys          # We use sys to get the max value of a float\n",
        "import pandas as pd # We only use pandas for displaying tables nicely\n",
        "pd.options.display.float_format = '{:,.3f}'.format"
      ],
      "metadata": {
        "id": "T9cAvA0GLkXh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ```World``` class and globals\n",
        "\n",
        "The ```World``` is a grid represented as a two-dimensional array of characters where each character can represent free space, an obstacle, or a terminal. Each non-obstacle cell is associated with a reward that an agent gets for moving to that cell (can be 0). The size of the world is _width_ $\\times$ _height_ characters.\n",
        "\n",
        "A _state_ is a tuple $(x,y)$.\n",
        "\n",
        "An empty world is created in the ```__init__``` method. Obstacles, rewards and terminals can then be added with ```add_obstacle``` and ```add_reward```.\n",
        "\n",
        "To calculate the next state of an agent (that is, an agent is in some state $s = (x,y)$ and performs and action, $a$), ```get_next_state()```should be called. It will only be relevant to call this function later on, when we do learning based on interaction with the environment and where an agent actually has to move.\n",
        "\n",
        "For now, you will only need the probabilities over next states given an action, $a$, that is, call ```get_state_transition_probabilities```."
      ],
      "metadata": {
        "id": "lTNglEH9FR8f"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PfR2S8j_LnZ3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Globals:\n",
        "ACTIONS = (\"up\", \"down\", \"left\", \"right\")\n",
        "\n",
        "# Rewards, terminals and obstacles are characters:\n",
        "REWARDS = {\" \": 0, \".\": 0.1, \"+\": 10, \"-\": -10}\n",
        "TERMINALS = (\"+\", \"-\") # Note a terminal should also have a reward assigned\n",
        "OBSTACLES = (\"#\")\n",
        "\n",
        "# Discount factor\n",
        "gamma = 1\n",
        "\n",
        "# The probability of a random move:\n",
        "rand_move_probability = 0\n",
        "\n",
        "class World:\n",
        "  def __init__(self, width, height):\n",
        "    self.width = width\n",
        "    self.height = height\n",
        "    # Create an empty world where the agent can move to all cells\n",
        "    self.grid = np.full((width, height), ' ', dtype='U1')\n",
        "\n",
        "  def add_obstacle(self, start_x, start_y, end_x=None, end_y=None):\n",
        "    \"\"\"\n",
        "    Create an obstacle in either a single cell or rectangle.\n",
        "    \"\"\"\n",
        "    if end_x == None: end_x = start_x\n",
        "    if end_y == None: end_y = start_y\n",
        "\n",
        "    self.grid[start_x:end_x + 1, start_y:end_y + 1] = OBSTACLES[0]\n",
        "\n",
        "  def add_reward(self, x, y, reward):\n",
        "    assert reward in REWARDS, f\"{reward} not in {REWARDS}\"\n",
        "    self.grid[x, y] = reward\n",
        "\n",
        "  def add_terminal(self, x, y, terminal):\n",
        "    assert terminal in TERMINALS, f\"{terminal} not in {TERMINALS}\"\n",
        "    self.grid[x, y] = terminal\n",
        "\n",
        "  def is_obstacle(self, x, y):\n",
        "    if x < 0 or x >= self.width or y < 0 or y >= self.height:\n",
        "      return True\n",
        "    else:\n",
        "      return self.grid[x ,y] in OBSTACLES\n",
        "\n",
        "  def is_terminal(self, x, y):\n",
        "    return self.grid[x ,y] in TERMINALS\n",
        "\n",
        "  def get_reward(self, x, y):\n",
        "    \"\"\"\n",
        "    Return the reward associated with a given location\n",
        "    \"\"\"\n",
        "    return REWARDS[self.grid[x, y]]\n",
        "\n",
        "  def get_next_state(self, current_state, action, deterministic=False):\n",
        "    \"\"\"\n",
        "    Get the next state given a current state and an action. Can eiter be\n",
        "    deterministic (no random actions) or non-deterministic,\n",
        "    where rand_move_probability determines the probability of ignoring the\n",
        "    action and performing a random move.\n",
        "    \"\"\"\n",
        "    assert action in ACTIONS, f\"Unknown acion {action} must be one of {ACTIONS}\"\n",
        "\n",
        "    x, y = current_state\n",
        "\n",
        "    # If our current state is a terminal, there is no next state\n",
        "    if self.grid[x, y] in TERMINALS:\n",
        "      return None\n",
        "\n",
        "    # Check of a random action should be performed:\n",
        "    if not deterministic and np.random.rand() < rand_move_probability:\n",
        "      action = np.random.choice(ACTIONS)\n",
        "\n",
        "    if action == \"up\":      y -= 1\n",
        "    elif action == \"down\":  y += 1\n",
        "    elif action == \"left\":  x -= 1\n",
        "    elif action == \"right\": x += 1\n",
        "\n",
        "    # If the next state is an obstacle, stay in the current state\n",
        "    return (x, y) if not self.is_obstacle(x, y) else current_state\n",
        "\n",
        "  def get_state_transition_probabilities(self, current_state, action):\n",
        "    \"\"\"\n",
        "    Returns a dict where key = state and value = probability given current state\n",
        "    is (x,y) and \"action\" is performed.\n",
        "    \"\"\"\n",
        "    assert action in ACTIONS, f\"Unknown acion {action} must be one of {ACTIONS}\"\n",
        "\n",
        "    x, y = current_state\n",
        "    if self.is_terminal(x, y):\n",
        "      return {}\n",
        "\n",
        "    next_state_probabilities = {}\n",
        "    # Since there is rand_move_probability of performing any action, we have to\n",
        "    # go through all actions and check what their next state would be:\n",
        "    for a in ACTIONS:\n",
        "      next_state = self.get_next_state((x, y), a, deterministic=True)\n",
        "      if a == action:\n",
        "        prob = 1 - rand_move_probability + rand_move_probability / len(ACTIONS)\n",
        "      else:\n",
        "        prob = rand_move_probability / len(ACTIONS)\n",
        "\n",
        "      if next_state in next_state_probabilities:\n",
        "        next_state_probabilities[next_state] += prob\n",
        "      else:\n",
        "        if prob > 0.0:\n",
        "          next_state_probabilities[next_state] = prob\n",
        "\n",
        "    return next_state_probabilities"
      ],
      "metadata": {
        "id": "wMAd6qTASn9u"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic examples: World, obstacles, rewards and terminals\n",
        "\n",
        "Below are some examples to illustrate how the ```World``` class works.\n",
        "\n",
        "First, we create a 4x4 world:"
      ],
      "metadata": {
        "id": "UxncUHvz1_Hb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "world = World(4, 4)\n",
        "\n",
        "# Note, that we have to transpose the 2D array (.T) for (x,y)\n",
        "# to match the convention when displayed\n",
        "print(world.grid.T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dL-XC6-aN6ss",
        "outputId": "1ca1d560-5d87-4820-eba5-93e3b94c20d7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obstacles and terminals are all represented as single characters:"
      ],
      "metadata": {
        "id": "L1C8CRt92wqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Obstacles: {OBSTACLES}\")\n",
        "print(f\"Terminals: {TERMINALS}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKlTP05N2xEe",
        "outputId": "5170d9c4-a30f-46c6-cadf-6da30def53e3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obstacles: #\n",
            "Terminals: ('+', '-')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rewards are also represented as characters in the world, but they have an associated value (note that defining a value for an empty space \"  \" is equivalent to the agent receiving that reward each time a move is made):"
      ],
      "metadata": {
        "id": "c30K2bDd3yyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Rewards: {REWARDS}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54TwLFOb3y-1",
        "outputId": "8501ede1-5773-409d-edd4-ba1031edad07"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rewards: {' ': 0, '.': 0.1, '+': 10, '-': -10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To assign rewards to terminal states, just use the same character in the ```REWARDS``` dict and in the ```TERMINALS``` tuple."
      ],
      "metadata": {
        "id": "LcG0mAYl4ux7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for t in TERMINALS:\n",
        "  print(f\"Terminal {t} has reward {REWARDS[t]}\")\n",
        "\n",
        "world.add_terminal(0, 0, \"+\")\n",
        "world.add_terminal(3, 3, \"-\")\n",
        "\n",
        "\n",
        "print(world.grid.T)\n",
        "\n",
        "# An alternative way of displaying the world using pandas:\n",
        "display(pd.DataFrame(world.grid.T))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "7MinV0Bg49dB",
        "outputId": "8e48ee49-439b-4a5d-b561-8d9a27f0a873"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Terminal + has reward 10\n",
            "Terminal - has reward -10\n",
            "[['+' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' '-']]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   0  1  2  3\n",
              "0  +         \n",
              "1            \n",
              "2            \n",
              "3           -"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-62e182c2-2642-4d57-8445-e14f39e9172e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>+</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>-</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-62e182c2-2642-4d57-8445-e14f39e9172e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-62e182c2-2642-4d57-8445-e14f39e9172e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-62e182c2-2642-4d57-8445-e14f39e9172e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ecd24f54-caba-43c9-a0f1-d91efc759185\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ecd24f54-caba-43c9-a0f1-d91efc759185')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ecd24f54-caba-43c9-a0f1-d91efc759185 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policies ($\\pi$)\n",
        "\n",
        "Recall that a policy, $\\pi(a|s) = \\Pr(A_t = a | S_t = s)$, maps states to action probabilities. In the code below, we let policies return the probabilities of each possible action given a state. States are $(x, y)$ coordinates and the policy must return action probabilities as a dict where the action is the ```key``` and the corresponding ```value``` is the probability of taking that action in the given state. Deterministic policies, for instance, return a dict with only one entry (e.g. ```{ \"up\": 1 } ``` if the action for the current state is ```up```).\n",
        "\n",
        "A random policy can be defined as follows:\n"
      ],
      "metadata": {
        "id": "94bkx57KdvGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def equiprobable_random_policy(x, y):\n",
        "  return { k : 1 / len(ACTIONS) for k in ACTIONS }\n",
        "\n",
        "# Example (since the action probabilities do not depend on the state in this\n",
        "# basic policy, we can just call it for state (0, 0)):\n",
        "print(equiprobable_random_policy(0, 0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vhf3hfg9eCYS",
        "outputId": "af326e4c-db54-4b82-977f-6d2ef451383e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Iterative policy evaluation\n",
        "\n",
        "\n",
        "Iterative policy evaluation takes a ```World```, a discount factor, $\\gamma$ (```gamma```, defined above in the ```World``` code cell), a policy, $\\pi$, and a threshold, $\\theta$ (```theta```), that determines when to stop the iteration. You can also specify a maximum number of iterations which can be useful for debugging using the ```max_iterations``` argument.\n",
        "\n",
        "**IMPORTANT:** Remember that in iterative policy evaluation, we just learn state values ($V_\\pi$) given a policy $\\pi$. We are **not** trying to learn a policy.\n",
        "\n",
        "(see page 74-75 of [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html) for an explanation and the algorithm)\n"
      ],
      "metadata": {
        "id": "nGK61Lk8dcd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def iterative_policy_evaluation(world, policy, theta=1e-5, max_iterations=1e3):\n",
        "\n",
        "  # Our initial estimates for all states in the world is 0:\n",
        "  V = np.full((world.width, world.height), 0.0)\n",
        "\n",
        "  while True:\n",
        "    # delta keeps track of the largest change in one iteration, so we set it to\n",
        "    # 0 at the start of each iteration:\n",
        "    delta = 0\n",
        "\n",
        "    # Loop over all states (x,y)\n",
        "    for y in range(world.height):\n",
        "      for x in range(world.width):\n",
        "        if not world.is_obstacle(x, y):\n",
        "          # Get action probabilities for the current state:\n",
        "          actions = policy(x, y)\n",
        "\n",
        "          # v is the new estimate that will be updated in the loop:\n",
        "          v = 0\n",
        "\n",
        "          # loop over all actions that our policy says that we can perform\n",
        "          # in the current state:\n",
        "          for action, action_prob in actions.items():\n",
        "            # For each action, get state transition probabilities and\n",
        "            # accumulate in v rewards weighted with action and state transition\n",
        "            # probabilities:\n",
        "            for (xi, yi), state_prob in world.get_state_transition_probabilities((x, y), action).items():\n",
        "              v += action_prob * state_prob * (world.get_reward(xi, yi) + gamma * V[xi, yi])\n",
        "\n",
        "          # update delta (largest change in estimate so far)\n",
        "          delta = max(delta, abs(v - V[x, y]))\n",
        "          V[x, y] = v\n",
        "\n",
        "    # check if current state value estimates are close enought to end:\n",
        "    if delta <= theta:\n",
        "      break\n",
        "\n",
        "    max_iterations -= 1\n",
        "    if max_iterations == 0:\n",
        "      break\n",
        "\n",
        "  # Return the state value estimates\n",
        "  return V\n"
      ],
      "metadata": {
        "id": "9fpxspw3b8U1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation of Example 4.1 from the book\n",
        "\n",
        "Below, you can see the implementation of Example 4.1 on page 76 in the book [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html)"
      ],
      "metadata": {
        "id": "MlZK9xU65iS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# World is 4x4\n",
        "world = World(4, 4)\n",
        "\n",
        "# Rewards are -1 for each move (including when hitting a terminal state, \"+\"):\n",
        "REWARDS = {\" \": -1, \"+\": -1}\n",
        "\n",
        "\n",
        "# Add terminal states in two corners\n",
        "world.add_terminal(0, 0, \"+\")\n",
        "world.add_terminal(3, 3, \"+\")\n",
        "\n",
        "print(world.grid.T)"
      ],
      "metadata": {
        "id": "mQG26oW05idR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d3998a6-4022-4bc6-e6c8-a87045efc1f5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['+' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' '+']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "V = iterative_policy_evaluation(world, equiprobable_random_policy)\n",
        "\n",
        "display(pd.DataFrame(V.T))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "tMudqC-OBUWo",
        "outputId": "24738d14-0186-445b-e1fa-12053ee05c48"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "        0       1       2       3\n",
              "0   0.000 -14.000 -20.000 -22.000\n",
              "1 -14.000 -18.000 -20.000 -20.000\n",
              "2 -20.000 -20.000 -18.000 -14.000\n",
              "3 -22.000 -20.000 -14.000   0.000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e7ddd85c-2a76-4b49-b398-24c60bd43af9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000</td>\n",
              "      <td>-14.000</td>\n",
              "      <td>-20.000</td>\n",
              "      <td>-22.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-14.000</td>\n",
              "      <td>-18.000</td>\n",
              "      <td>-20.000</td>\n",
              "      <td>-20.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-20.000</td>\n",
              "      <td>-20.000</td>\n",
              "      <td>-18.000</td>\n",
              "      <td>-14.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-22.000</td>\n",
              "      <td>-20.000</td>\n",
              "      <td>-14.000</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e7ddd85c-2a76-4b49-b398-24c60bd43af9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e7ddd85c-2a76-4b49-b398-24c60bd43af9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e7ddd85c-2a76-4b49-b398-24c60bd43af9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-22daee4f-d3f2-4628-95a8-933b0d6b4a05\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-22daee4f-d3f2-4628-95a8-933b0d6b4a05')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-22daee4f-d3f2-4628-95a8-933b0d6b4a05 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise - policy and discount factor\n",
        "\n",
        "Experiment with example 4.1: what effect does it have to change the policy, e.g. so that an agent always goes left or always goes right? What effect does it have on state values to change the value of the discount factor (```gamma```)?"
      ],
      "metadata": {
        "id": "QNagzYkLaqAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: implement your code here"
      ],
      "metadata": {
        "id": "krYhqYtTOGhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try to write a policy that is deterministic, but where the action performed differs between states. You can implement it as a two-dimensional array with the dimensions corresponding to the world dimensions and have each entry be an action for the corresponding state."
      ],
      "metadata": {
        "id": "2CtRf7JInBq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: implement you code here\n"
      ],
      "metadata": {
        "id": "pel0dbaWnh1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise - stochasticity\n",
        "\n",
        "You can adjust the degree of stochasticity in the environment by setting the global variable ```rand_move_probability``` (the probability of the world ignoring an action and performing a random move instead). What effect does stochasticity have on the state-value estimates?\n"
      ],
      "metadata": {
        "id": "SHDCfB_uiuxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: implement you code here"
      ],
      "metadata": {
        "id": "ddMEnWvPcCuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise - robot, cake and mouse trap\n",
        "\n",
        "Implement a robot, cake and mouse trap example and compute state value estimates under different policies (equiprobable, always right, always right:50% or up:50%) with and without stochasticity.\n"
      ],
      "metadata": {
        "id": "pHC0cosVjU4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: implement you code here"
      ],
      "metadata": {
        "id": "ujejW_93jzQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise - action-value function\n",
        "\n",
        "Based on a set of calculated state values, try to implement an action value function, that is $q_\\pi(s, a)$ (if in doubt, see page 78 in [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html)). Note: you have to use the ```get_state_transition_probabilities()``` method on ```World``` to be able to handle stochastic environments where performing ```a``` does not lead to a deterministic outcome."
      ],
      "metadata": {
        "id": "t_kjVW7UkDhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def action_value(world, V, state, action):\n",
        "  # TODO: implement your code here\n",
        "  pass\n"
      ],
      "metadata": {
        "id": "qiCGsZ7klPzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise - policy iteration\n",
        "\n",
        "You are now ready to implement policy iteration. That is, first estimate state values under a given policy, then improve the policy based on those estimates and action values, estimate state values again, and so on. See page 80 in [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html)\n",
        "\n",
        "You will need an explicit representation of your policy that you can easily change.\n",
        "\n",
        "Test your implementation and print out the policies found.\n"
      ],
      "metadata": {
        "id": "hveeaq7zn2g5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement your code here"
      ],
      "metadata": {
        "id": "h9WW80CWoFPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise - value iteration\n",
        "\n",
        "Value iteration is much more effecient than policy iteration. Implement value iteration below. See page 83 in [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html).\n",
        "\n",
        "Test your implementation and display the policies found (i.e., a grid with the perferred action in each cell)."
      ],
      "metadata": {
        "id": "ijYaKkBloGdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement your code here"
      ],
      "metadata": {
        "id": "0lw1ZXVroJUa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}