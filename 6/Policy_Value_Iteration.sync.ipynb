{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "761d3b23",
   "metadata": {},
   "source": [
    "# Policy iteration and value iteration\n",
    "\n",
    "In this notebook, you will implement different dynamic programming approaches described in [Sutton and Barto's book, Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html). A grid ```World``` class and policy iteration has been implemented. Feel free to add more actions, rewards and/or terminals, or to modify the code to suit your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7558fd13",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d69cabab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "pd.options.display.float_format = \"{:,.3f}\".format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3bbebe",
   "metadata": {},
   "source": [
    "### `World` class and globals\n",
    "\n",
    "The `World` is a grid represented as a two-dimensional array of characters where each character can represent free space, an obstacle, or a terminal. Each non-obstacle cell is associated with a reward that an agent gets for moving to that cell (can be 0). The size of the world is _width_ $\\times$ _height_ characters.\n",
    "\n",
    "A _state_ is a tuple $(x,y)$.\n",
    "\n",
    "An empty world is created in the ```__init__``` method. Obstacles, rewards and terminals can then be added with ```add_obstacle``` and ```add_reward```.\n",
    "\n",
    "To calculate the next state of an agent (that is, an agent is in some state $s = (x,y)$ and performs and action, $a$), ```get_next_state()```should be called. It will only be relevant to call this function later on, when we do learning based on interaction with the environment and where an agent actually has to move.\n",
    "\n",
    "For now, you will only need the probabilities over next states given an action, $a$, that is, call ```get_state_transition_probabilities```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cb3430a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals:\n",
    "ACTIONS = (\"up\", \"down\", \"left\", \"right\")\n",
    "\n",
    "# Rewards, terminals and obstacles are characters:\n",
    "REWARDS = {\" \": 0, \".\": 0.1, \"+\": 10, \"-\": -10}\n",
    "TERMINALS = (\"+\", \"-\")  # Note a terminal should also have a reward assigned\n",
    "OBSTACLES = \"#\"\n",
    "\n",
    "# Discount factor\n",
    "gamma = 1\n",
    "\n",
    "# The probability of a random move:\n",
    "rand_move_probability = 0\n",
    "\n",
    "\n",
    "class World:\n",
    "    def __init__(self, width, height):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        # Create an empty world where the agent can move to all cells\n",
    "        self.grid = np.full((width, height), \" \", dtype=\"U1\")\n",
    "\n",
    "    def add_obstacle(self, start_x, start_y, end_x=None, end_y=None):\n",
    "        \"\"\"\n",
    "        Create an obstacle in either a single cell or rectangle.\n",
    "        \"\"\"\n",
    "        if end_x is None:\n",
    "            end_x = start_x\n",
    "        if end_y is None:\n",
    "            end_y = start_y\n",
    "\n",
    "        self.grid[start_x : end_x + 1, start_y : end_y + 1] = OBSTACLES[0]\n",
    "\n",
    "    def add_reward(self, x, y, reward):\n",
    "        assert reward in REWARDS, f\"{reward} not in {REWARDS}\"\n",
    "        self.grid[x, y] = reward\n",
    "\n",
    "    def add_terminal(self, x, y, terminal):\n",
    "        assert terminal in TERMINALS, f\"{terminal} not in {TERMINALS}\"\n",
    "        self.grid[x, y] = terminal\n",
    "\n",
    "    def is_obstacle(self, x, y):\n",
    "        if x < 0 or x >= self.width or y < 0 or y >= self.height:\n",
    "            return True\n",
    "        else:\n",
    "            return self.grid[x, y] in OBSTACLES\n",
    "\n",
    "    def is_terminal(self, x, y):\n",
    "        return self.grid[x, y] in TERMINALS\n",
    "\n",
    "    def get_reward(self, x, y):\n",
    "        \"\"\"\n",
    "        Return the reward associated with a given location\n",
    "        \"\"\"\n",
    "        return REWARDS[self.grid[x, y]]\n",
    "\n",
    "    def get_next_state(self, current_state, action, deterministic=False):\n",
    "        \"\"\"\n",
    "        Get the next state given a current state and an action. Can eiter be\n",
    "        deterministic (no random actions) or non-deterministic,\n",
    "        where rand_move_probability determines the probability of ignoring the\n",
    "        action and performing a random move.\n",
    "        \"\"\"\n",
    "        assert action in ACTIONS, f\"Unknown acion {action} must be one of {ACTIONS}\"\n",
    "\n",
    "        x, y = current_state\n",
    "\n",
    "        # If our current state is a terminal, there is no next state\n",
    "        if self.grid[x, y] in TERMINALS:\n",
    "            return None\n",
    "\n",
    "        # Check of a random action should be performed:\n",
    "        if not deterministic and np.random.rand() < rand_move_probability:\n",
    "            action = np.random.choice(ACTIONS)\n",
    "\n",
    "        if action == \"up\":\n",
    "            y -= 1\n",
    "        elif action == \"down\":\n",
    "            y += 1\n",
    "        elif action == \"left\":\n",
    "            x -= 1\n",
    "        elif action == \"right\":\n",
    "            x += 1\n",
    "\n",
    "        # If the next state is an obstacle, stay in the current state\n",
    "        return (x, y) if not self.is_obstacle(x, y) else current_state\n",
    "\n",
    "    def get_state_transition_probabilities(self, current_state, action):\n",
    "        \"\"\"\n",
    "        Returns a dict where key = state and value = probability given current state\n",
    "        is (x,y) and \"action\" is performed.\n",
    "        \"\"\"\n",
    "        assert action in ACTIONS, f\"Unknown acion {action} must be one of {ACTIONS}\"\n",
    "\n",
    "        x, y = current_state\n",
    "        if self.is_terminal(x, y):\n",
    "            return {}\n",
    "\n",
    "        next_state_probabilities = {}\n",
    "        # Since there is rand_move_probability of performing any action, we have to\n",
    "        # go through all actions and check what their next state would be:\n",
    "        for a in ACTIONS:\n",
    "            next_state = self.get_next_state((x, y), a, deterministic=True)\n",
    "            if a == action:\n",
    "                prob = 1 - rand_move_probability + rand_move_probability / len(ACTIONS)\n",
    "            else:\n",
    "                prob = rand_move_probability / len(ACTIONS)\n",
    "\n",
    "            if next_state in next_state_probabilities:\n",
    "                next_state_probabilities[next_state] += prob\n",
    "            else:\n",
    "                if prob > 0.0:\n",
    "                    next_state_probabilities[next_state] = prob\n",
    "\n",
    "        return next_state_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b506dd",
   "metadata": {},
   "source": [
    "## Basic examples: World, obstacles, rewards and terminals\n",
    "\n",
    "Below are some examples to illustrate how the ```World``` class works.\n",
    "\n",
    "First, we create a 4x4 world:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "52b7b13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "world = World(4, 4)\n",
    "\n",
    "# Note, that we have to transpose the 2D array (.T) for (x,y)\n",
    "# to match the convention when displayed\n",
    "print(world.grid.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174d524d",
   "metadata": {},
   "source": [
    "Obstacles and terminals are all represented as single characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d4158ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obstacles: #\n",
      "Terminals: ('+', '-')\n"
     ]
    }
   ],
   "source": [
    "print(f\"Obstacles: {OBSTACLES}\")\n",
    "print(f\"Terminals: {TERMINALS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2f2c69",
   "metadata": {},
   "source": [
    "Rewards are also represented as characters in the world, but they have an associated value (note that defining a value for an empty space \"  \" is equivalent to the agent receiving that reward each time a move is made):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "27a5455a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards: {' ': 0, '.': 0.1, '+': 10, '-': -10}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Rewards: {REWARDS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61493d93",
   "metadata": {},
   "source": [
    "To assign rewards to terminal states, just use the same character in the `REWARDS` dict and in the `TERMINALS` tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c0511953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminal + has reward 10\n",
      "Terminal - has reward -10\n",
      "[['+' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' '-']]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>+</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3\n",
       "0  +         \n",
       "1            \n",
       "2            \n",
       "3           -"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for t in TERMINALS:\n",
    "    print(f\"Terminal {t} has reward {REWARDS[t]}\")\n",
    "\n",
    "world.add_terminal(0, 0, \"+\")\n",
    "world.add_terminal(3, 3, \"-\")\n",
    "\n",
    "print(world.grid.T)\n",
    "\n",
    "# An alternative way of displaying the world using pandas:\n",
    "display(pd.DataFrame(world.grid.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65755bf1",
   "metadata": {},
   "source": [
    "## Policies ($\\pi$)\n",
    "\n",
    "Recall that a policy, $\\pi(a|s) = \\Pr(A_t = a | S_t = s)$, maps states to action probabilities. In the code below, we let policies return the probabilities of each possible action given a state. States are $(x, y)$ coordinates and the policy must return action probabilities as a dict where the action is the ```key``` and the corresponding ```value``` is the probability of taking that action in the given state. Deterministic policies, for instance, return a dict with only one entry (e.g. ```{ \"up\": 1 } ``` if the action for the current state is ```up```).\n",
    "\n",
    "A random policy can be defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5b30c36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25}\n"
     ]
    }
   ],
   "source": [
    "def equiprobable_random_policy(x, y):\n",
    "    return {k: 1 / len(ACTIONS) for k in ACTIONS}\n",
    "\n",
    "\n",
    "# Example (since the action probabilities do not depend on the state in this\n",
    "# basic policy, we can just call it for state (0, 0)):\n",
    "print(equiprobable_random_policy(0, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ce0b17",
   "metadata": {},
   "source": [
    "## Iterative policy evaluation\n",
    "\n",
    "Iterative policy evaluation takes a ```World```, a discount factor, $\\gamma$ (```gamma```, defined above in the ```World``` code cell), a policy, $\\pi$, and a threshold, $\\theta$ (```theta```), that determines when to stop the iteration. You can also specify a maximum number of iterations which can be useful for debugging using the ```max_iterations``` argument.\n",
    "\n",
    "**IMPORTANT:** Remember that in iterative policy evaluation, we just learn state values ($V_\\pi$) given a policy $\\pi$. We are **not** trying to learn a policy.\n",
    "\n",
    "(see page 74-75 of [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html) for an explanation and the algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0b4cee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_policy_evaluation(world, policy, theta=1e-5, max_iterations=1e3):\n",
    "    # Our initial estimates for all states in the world is 0:\n",
    "    V = np.full((world.width, world.height), 0.0)\n",
    "\n",
    "    while True:\n",
    "        # delta keeps track of the largest change in one iteration, so we set it to\n",
    "        # 0 at the start of each iteration:\n",
    "        delta = 0\n",
    "\n",
    "        # Loop over all states (x,y)\n",
    "        for y in range(world.height):\n",
    "            for x in range(world.width):\n",
    "                if not world.is_obstacle(x, y):\n",
    "                    # Get action probabilities for the current state:\n",
    "                    actions = policy(x, y)\n",
    "\n",
    "                    # v is the new estimate that will be updated in the loop:\n",
    "                    v = 0\n",
    "\n",
    "                    # loop over all actions that our policy says that we can perform\n",
    "                    # in the current state:\n",
    "                    for action, action_prob in actions.items():\n",
    "                        # For each action, get state transition probabilities and\n",
    "                        # accumulate in v rewards weighted with action and state transition\n",
    "                        # probabilities:\n",
    "                        for (\n",
    "                            xi,\n",
    "                            yi,\n",
    "                        ), state_prob in world.get_state_transition_probabilities(\n",
    "                            (x, y), action\n",
    "                        ).items():\n",
    "                            v += (\n",
    "                                action_prob\n",
    "                                * state_prob\n",
    "                                * (world.get_reward(xi, yi) + gamma * V[xi, yi])\n",
    "                            )\n",
    "\n",
    "                    # update delta (largest change in estimate so far)\n",
    "                    delta = max(delta, abs(v - V[x, y]))\n",
    "                    V[x, y] = v\n",
    "\n",
    "        # check if current state value estimates are close enought to end:\n",
    "        if delta <= theta:\n",
    "            break\n",
    "\n",
    "        max_iterations -= 1\n",
    "        if max_iterations == 0:\n",
    "            break\n",
    "\n",
    "    # Return the state value estimates\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b66c2be",
   "metadata": {},
   "source": [
    "## Implementation of Example 4.1 from the book\n",
    "\n",
    "Below, you can see the implementation of Example 4.1 on page 76 in the book [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c7df4675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['+' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' '+']]\n"
     ]
    }
   ],
   "source": [
    "# World is 4x4\n",
    "world = World(4, 4)\n",
    "\n",
    "# Rewards are -1 for each move (including when hitting a terminal state, \"+\"):\n",
    "REWARDS = {\" \": -1, \"+\": -1}\n",
    "\n",
    "\n",
    "# Add terminal states in two corners\n",
    "world.add_terminal(0, 0, \"+\")\n",
    "world.add_terminal(3, 3, \"+\")\n",
    "\n",
    "print(world.grid.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a1ed4f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>-14.000</td>\n",
       "      <td>-20.000</td>\n",
       "      <td>-22.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-14.000</td>\n",
       "      <td>-18.000</td>\n",
       "      <td>-20.000</td>\n",
       "      <td>-20.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-20.000</td>\n",
       "      <td>-20.000</td>\n",
       "      <td>-18.000</td>\n",
       "      <td>-14.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-22.000</td>\n",
       "      <td>-20.000</td>\n",
       "      <td>-14.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0       1       2       3\n",
       "0   0.000 -14.000 -20.000 -22.000\n",
       "1 -14.000 -18.000 -20.000 -20.000\n",
       "2 -20.000 -20.000 -18.000 -14.000\n",
       "3 -22.000 -20.000 -14.000   0.000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "V = iterative_policy_evaluation(world, equiprobable_random_policy)\n",
    "\n",
    "display(pd.DataFrame(V.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8601135a",
   "metadata": {},
   "source": [
    "## Exercise - policy and discount factor\n",
    "\n",
    "Experiment with example 4.1: what effect does it have to change the policy, e.g. so that an agent always goes left or always goes right? What effect does it have on state values to change the value of the discount factor (```gamma```)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0b848d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca5bacc",
   "metadata": {},
   "source": [
    "Try to write a policy that is deterministic, but where the action performed differs between states. You can implement it as a two-dimensional array with the dimensions corresponding to the world dimensions and have each entry be an action for the corresponding state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6c9eeef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement you code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d95e313",
   "metadata": {},
   "source": [
    "## Exercise - stochasticity\n",
    "\n",
    "You can adjust the degree of stochasticity in the environment by setting the global variable ```rand_move_probability``` (the probability of the world ignoring an action and performing a random move instead). What effect does stochasticity have on the state-value estimates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b992c279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement you code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f67d12",
   "metadata": {},
   "source": [
    "## Exercise - robot, cake and mouse trap\n",
    "\n",
    "Implement a robot, cake and mouse trap example and compute state value estimates under different policies (equiprobable, always right, always right:50% or up:50%) with and without stochasticity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "60ae0462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement you code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f309383c",
   "metadata": {},
   "source": [
    "## Exercise - action-value function\n",
    "\n",
    "Based on a set of calculated state values, try to implement an action value function, that is $q_\\pi(s, a)$ (if in doubt, see page 78 in [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html)). Note: you have to use the ```get_state_transition_probabilities()``` method on ```World``` to be able to handle stochastic environments where performing ```a``` does not lead to a deterministic outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eb7fe94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_value(world, V, state, action):\n",
    "    # TODO: implement your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fa653c",
   "metadata": {},
   "source": [
    "## Exercise - policy iteration\n",
    "\n",
    "You are now ready to implement policy iteration. That is, first estimate state values under a given policy, then improve the policy based on those estimates and action values, estimate state values again, and so on. See page 80 in [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html)\n",
    "\n",
    "You will need an explicit representation of your policy that you can easily change.\n",
    "\n",
    "Test your implementation and print out the policies found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7c3a810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement you code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b5eacd",
   "metadata": {},
   "source": [
    "## Exercise - value iteration\n",
    "\n",
    "Value iteration is much more effecient than policy iteration. Implement value iteration below. See page 83 in [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html).\n",
    "\n",
    "Test your implementation and display the policies found (i.e., a grid with the perferred action in each cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e599d1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement you code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
