{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bee904f5",
   "metadata": {},
   "source": [
    "# Monte Carlo approaches to prediction and control\n",
    "\n",
    "In this notebook, you will implement the Monte Carlo approaches to prediction and control described in [Sutton and Barto's book, Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html). We will use the grid ```World``` class from the previous lecture, but now without relying on knowledge of the task dynamics, that is, without relying on knowledge about transition probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b490c0a",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d851c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/balling/projects/AI/env/lib/python3.9/site-packages (2.0.2)\n",
      "Requirement already satisfied: pandas in /Users/balling/projects/AI/env/lib/python3.9/site-packages (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/balling/projects/AI/env/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/balling/projects/AI/env/lib/python3.9/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/balling/projects/AI/env/lib/python3.9/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/balling/projects/AI/env/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install numpy pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28d588e",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d53ac59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys  # We use sys to get the max value of a float\n",
    "import pandas as pd  # We only use pandas for displaying tables nicely\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "from pandas.core.arrays.arrow.accessors import ListAccessor\n",
    "\n",
    "pd.options.display.float_format = \"{:,.3f}\".format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954f35da",
   "metadata": {},
   "source": [
    "### ```World``` class and globals\n",
    "\n",
    "The ```World``` is a grid represented as a two-dimensional array of characters where each character can represent free space, an obstacle, or a terminal. Each non-obstacle cell is associated with a reward that an agent gets for moving to that cell (can be 0). The size of the world is _width_ $\\times$ _height_ characters.\n",
    "\n",
    "A _state_ is a tuple $(x,y)$.\n",
    "\n",
    "An empty world is created in the ```__init__``` method. Obstacles, rewards and terminals can then be added with ```add_obstacle``` and ```add_reward```.\n",
    "\n",
    "To calculate the next state of an agent (that is, an agent is in some state $s = (x,y)$ and performs and action, $a$), ```get_next_state()```should be called.\n",
    "\n",
    "__Note that ```get_state_transition_probabilities``` has been removed and an agent must now rely on experience interacting with a world to learn.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2916bff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals:\n",
    "ACTIONS = (\"up\", \"down\", \"left\", \"right\")\n",
    "\n",
    "# Rewards, terminals and obstacles are characters:\n",
    "REWARDS = {\" \": 0, \".\": 0.1, \"+\": 10, \"-\": -10}\n",
    "TERMINALS = (\"+\", \"-\")  # Note a terminal should also have a reward assigned\n",
    "OBSTACLES = \"#\"\n",
    "\n",
    "# Discount factor\n",
    "gamma = 1\n",
    "\n",
    "# The probability of a random move:\n",
    "rand_move_probability = 0\n",
    "\n",
    "\n",
    "class World:\n",
    "    def __init__(self, width, height):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        # Create an empty world where the agent can move to all cells\n",
    "        self.grid = np.full((width, height), \" \", dtype=\"U1\")\n",
    "\n",
    "    def add_obstacle(self, start_x, start_y, end_x=None, end_y=None):\n",
    "        \"\"\"\n",
    "        Create an obstacle in either a single cell or rectangle.\n",
    "        \"\"\"\n",
    "        if end_x == None:\n",
    "            end_x = start_x\n",
    "        if end_y == None:\n",
    "            end_y = start_y\n",
    "\n",
    "        self.grid[start_x : end_x + 1, start_y : end_y + 1] = OBSTACLES[0]\n",
    "\n",
    "    def add_reward(self, x, y, reward):\n",
    "        assert reward in REWARDS, f\"{reward} not in {REWARDS}\"\n",
    "        self.grid[x, y] = reward\n",
    "\n",
    "    def add_terminal(self, x, y, terminal):\n",
    "        assert terminal in TERMINALS, f\"{terminal} not in {TERMINALS}\"\n",
    "        self.grid[x, y] = terminal\n",
    "\n",
    "    def is_obstacle(self, x, y):\n",
    "        if x < 0 or x >= self.width or y < 0 or y >= self.height:\n",
    "            return True\n",
    "        else:\n",
    "            return self.grid[x, y] in OBSTACLES\n",
    "\n",
    "    def is_terminal(self, x, y):\n",
    "        return self.grid[x, y] in TERMINALS\n",
    "\n",
    "    def get_reward(self, x, y):\n",
    "        \"\"\"\n",
    "        Return the reward associated with a given location\n",
    "        \"\"\"\n",
    "        return REWARDS[self.grid[x, y]]\n",
    "\n",
    "    def get_next_state(self, current_state, action):\n",
    "        \"\"\"\n",
    "        Get the next state given a current state and an action. The outcome can be\n",
    "        stochastic  where rand_move_probability determines the probability of\n",
    "        ignoring the action and performing a random move.\n",
    "        \"\"\"\n",
    "        assert action in ACTIONS, f\"Unknown acion {action} must be one of {ACTIONS}\"\n",
    "\n",
    "        x, y = current_state\n",
    "\n",
    "        # If our current state is a terminal, there is no next state\n",
    "        if self.grid[x, y] in TERMINALS:\n",
    "            return None\n",
    "\n",
    "        # Check of a random action should be performed:\n",
    "        if np.random.rand() < rand_move_probability:\n",
    "            action = np.random.choice(ACTIONS)\n",
    "\n",
    "        if action == \"up\":\n",
    "            y -= 1\n",
    "        elif action == \"down\":\n",
    "            y += 1\n",
    "        elif action == \"left\":\n",
    "            x -= 1\n",
    "        elif action == \"right\":\n",
    "            x += 1\n",
    "\n",
    "        # If the next state is an obstacle, stay in the current state\n",
    "        return (x, y) if not self.is_obstacle(x, y) else current_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c26e2e",
   "metadata": {},
   "source": [
    "## Basic example: Generating episodes\n",
    "\n",
    "An episode is the series of states, actions and rewards reflecting an agent's experience interacting with the environment. An episode starts with an agent being placed at some initial state and continues till the agent reaches a terminal state.  To generate episodes, we first need a world and a policy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c978014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ']\n",
      " [' ' ' ']\n",
      " [' ' '+']]\n"
     ]
    }
   ],
   "source": [
    "world = World(2, 3)\n",
    "\n",
    "# Since we only focus on episodic tasks, we must have a terminal state that the\n",
    "# agent eventually reaches\n",
    "world.add_terminal(1, 2, \"+\")\n",
    "\n",
    "\n",
    "def equiprobable_random_policy(x, y):\n",
    "    return {k: 1 / len(ACTIONS) for k in ACTIONS}\n",
    "\n",
    "\n",
    "print(world.grid.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abad9deb",
   "metadata": {},
   "source": [
    "To generate an episode, we need to provide a ```World```, a policy, and a start state.\n",
    "\n",
    "In each step, we do the following:\n",
    "1. perform one of the actions (weighted random) returned by the policy for the giving state\n",
    "2. get the reward and add a new entry to the episode $[S_t, A_t, R_{t+1}]$\n",
    "3. move the agent to the next state\n",
    "\n",
    "When a terminal state is reached, we return all the $[[S_0, A_0, R_1], ..., [S_{T}, A_T, R_{T+1}]]$ observed in the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a599f403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(world: World, policy, start_state):\n",
    "    current_state = start_state\n",
    "    episode = []\n",
    "    while not world.is_terminal(*current_state):\n",
    "        # Get the possible actions and their probabilities that our policy says\n",
    "        # that the agent should perform in the current state:\n",
    "        possible_actions = policy(*current_state)\n",
    "\n",
    "        # Pick a weighted random action:\n",
    "        action = random.choices(\n",
    "            population=list(possible_actions.keys()),\n",
    "            weights=possible_actions.values(),\n",
    "            k=1,\n",
    "        )\n",
    "\n",
    "        # Get the next state from the world\n",
    "        next_state = world.get_next_state(current_state, action[0])\n",
    "\n",
    "        # Get the reward for performing the action\n",
    "        reward = world.get_reward(*next_state)\n",
    "\n",
    "        # Save the state, action and reward for this time step in our episode\n",
    "        episode.append([current_state, action[0], reward])\n",
    "\n",
    "        # Move the agent to the new state\n",
    "        current_state = next_state\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b08703",
   "metadata": {},
   "source": [
    "Now, we can try to generate a couple of episodes and print the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdb7cd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0:\n",
      "    State Action  Reward\n",
      "0  (0, 0)  right       0\n",
      "1  (1, 0)   down       0\n",
      "2  (1, 1)     up       0\n",
      "3  (1, 0)     up       0\n",
      "4  (1, 0)   down       0\n",
      "5  (1, 1)   down      10\n",
      "\n",
      "Episode 1:\n",
      "     State Action  Reward\n",
      "0   (0, 0)  right       0\n",
      "1   (1, 0)  right       0\n",
      "2   (1, 0)  right       0\n",
      "3   (1, 0)   down       0\n",
      "4   (1, 1)     up       0\n",
      "5   (1, 0)   left       0\n",
      "6   (0, 0)     up       0\n",
      "7   (0, 0)  right       0\n",
      "8   (1, 0)  right       0\n",
      "9   (1, 0)   left       0\n",
      "10  (0, 0)     up       0\n",
      "11  (0, 0)  right       0\n",
      "12  (1, 0)   down       0\n",
      "13  (1, 1)   left       0\n",
      "14  (0, 1)     up       0\n",
      "15  (0, 0)   down       0\n",
      "16  (0, 1)     up       0\n",
      "17  (0, 0)     up       0\n",
      "18  (0, 0)   left       0\n",
      "19  (0, 0)     up       0\n",
      "20  (0, 0)  right       0\n",
      "21  (1, 0)   down       0\n",
      "22  (1, 1)     up       0\n",
      "23  (1, 0)   down       0\n",
      "24  (1, 1)  right       0\n",
      "25  (1, 1)     up       0\n",
      "26  (1, 0)     up       0\n",
      "27  (1, 0)  right       0\n",
      "28  (1, 0)   down       0\n",
      "29  (1, 1)   down      10\n",
      "\n",
      "Episode 2:\n",
      "    State Action  Reward\n",
      "0  (0, 0)  right       0\n",
      "1  (1, 0)   down       0\n",
      "2  (1, 1)     up       0\n",
      "3  (1, 0)   left       0\n",
      "4  (0, 0)   down       0\n",
      "5  (0, 1)   down       0\n",
      "6  (0, 2)   down       0\n",
      "7  (0, 2)   down       0\n",
      "8  (0, 2)  right      10\n",
      "\n",
      "Episode 3:\n",
      "     State Action  Reward\n",
      "0   (0, 0)     up       0\n",
      "1   (0, 0)   down       0\n",
      "2   (0, 1)  right       0\n",
      "3   (1, 1)     up       0\n",
      "4   (1, 0)   down       0\n",
      "5   (1, 1)     up       0\n",
      "6   (1, 0)     up       0\n",
      "7   (1, 0)   left       0\n",
      "8   (0, 0)  right       0\n",
      "9   (1, 0)  right       0\n",
      "10  (1, 0)   left       0\n",
      "11  (0, 0)     up       0\n",
      "12  (0, 0)  right       0\n",
      "13  (1, 0)  right       0\n",
      "14  (1, 0)   down       0\n",
      "15  (1, 1)  right       0\n",
      "16  (1, 1)     up       0\n",
      "17  (1, 0)  right       0\n",
      "18  (1, 0)   down       0\n",
      "19  (1, 1)     up       0\n",
      "20  (1, 0)  right       0\n",
      "21  (1, 0)  right       0\n",
      "22  (1, 0)     up       0\n",
      "23  (1, 0)   down       0\n",
      "24  (1, 1)   left       0\n",
      "25  (0, 1)  right       0\n",
      "26  (1, 1)  right       0\n",
      "27  (1, 1)  right       0\n",
      "28  (1, 1)  right       0\n",
      "29  (1, 1)   left       0\n",
      "30  (0, 1)  right       0\n",
      "31  (1, 1)   left       0\n",
      "32  (0, 1)  right       0\n",
      "33  (1, 1)   left       0\n",
      "34  (0, 1)     up       0\n",
      "35  (0, 0)  right       0\n",
      "36  (1, 0)     up       0\n",
      "37  (1, 0)   down       0\n",
      "38  (1, 1)   left       0\n",
      "39  (0, 1)     up       0\n",
      "40  (0, 0)     up       0\n",
      "41  (0, 0)  right       0\n",
      "42  (1, 0)     up       0\n",
      "43  (1, 0)  right       0\n",
      "44  (1, 0)   left       0\n",
      "45  (0, 0)   down       0\n",
      "46  (0, 1)   down       0\n",
      "47  (0, 2)  right      10\n",
      "\n",
      "Episode 4:\n",
      "    State Action  Reward\n",
      "0  (0, 0)     up       0\n",
      "1  (0, 0)  right       0\n",
      "2  (1, 0)  right       0\n",
      "3  (1, 0)     up       0\n",
      "4  (1, 0)   left       0\n",
      "5  (0, 0)  right       0\n",
      "6  (1, 0)   down       0\n",
      "7  (1, 1)   down      10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Episode {i}:\")\n",
    "    episode = generate_episode(world, equiprobable_random_policy, (0, 0))\n",
    "    print(pd.DataFrame(episode, columns=[\"State\", \"Action\", \"Reward\"]), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19535fa4",
   "metadata": {},
   "source": [
    "### Exercise: Implement Monte Carlo-based prediction for state values\n",
    "\n",
    "You should implement first-visit MC prediction for estimating $V≈v_\\pi$. See page 92 of [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8d4ad36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_fv_prediction(world: World, start_state, policy, V, returns, gamma):\n",
    "    # Generate a random start state\n",
    "    episode = generate_episode(world, policy, start_state)\n",
    "\n",
    "    G = 0\n",
    "    for idx in reversed(range(len(episode))):\n",
    "        state, _, reward = episode[idx]\n",
    "        G = gamma * G + reward\n",
    "        if state not in [s for s, _, _ in episode[:idx]]:\n",
    "            returns[state].append(G)\n",
    "            V[state] = np.average(returns[state])\n",
    "\n",
    "    return V, returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c6e537",
   "metadata": {},
   "source": [
    "First, try your algorithm on the small $2\\times3$ world above using an equiprobable policy and $\\gamma = 0.9$. Depending on the number of episodes you use, you should get close to the true values:\n",
    "\n",
    "<table class=\"dataframe\" border=\"1\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>0</th>\n",
    "      <th>1</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>3.283</td>\n",
    "      <td>3.616</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>4.409</td>\n",
    "      <td>5.556</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>6.349</td>\n",
    "      <td>0.000</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f3535e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1\n",
       "0      \n",
       "1      \n",
       "2     +"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.284</td>\n",
       "      <td>3.613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.421</td>\n",
       "      <td>5.591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.426</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0 3.284 3.613\n",
       "1 4.421 5.591\n",
       "2 6.426 0.000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "world = World(2, 3)\n",
    "world.add_terminal(1, 2, \"+\")\n",
    "\n",
    "V = np.zeros((world.width, world.height))\n",
    "returns = {(i, j): [] for i in range(world.width) for j in range(world.height)}\n",
    "\n",
    "nb_episodes = 20000\n",
    "\n",
    "for i in range(nb_episodes):\n",
    "    V, returns = mc_fv_prediction(\n",
    "        world=world,\n",
    "        start_state=(0, 0),\n",
    "        policy=equiprobable_random_policy,\n",
    "        V=V,\n",
    "        returns=returns,\n",
    "        gamma=0.9,\n",
    "    )\n",
    "display(pd.DataFrame(world.grid.T))\n",
    "display(pd.DataFrame(V.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a0ef10",
   "metadata": {},
   "source": [
    "Try to run your MC prediction code on worlds of different sizes (be careful not to make your world too large or you should have multiple terminals that an agent is likely to hit, otherwise it may take too long). You can try to change the policy as well, but rememeber that the agent **must** eventually reach a terminal state under any policy that you try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efe0bc29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>+</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5  6\n",
       "0                     \n",
       "1                     \n",
       "2     -               \n",
       "3                     \n",
       "4        +            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.335</td>\n",
       "      <td>-2.330</td>\n",
       "      <td>-1.526</td>\n",
       "      <td>-0.732</td>\n",
       "      <td>-0.333</td>\n",
       "      <td>-0.158</td>\n",
       "      <td>-0.051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.290</td>\n",
       "      <td>-4.260</td>\n",
       "      <td>-2.082</td>\n",
       "      <td>-0.855</td>\n",
       "      <td>-0.253</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-4.680</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-2.648</td>\n",
       "      <td>-0.460</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.856</td>\n",
       "      <td>-1.855</td>\n",
       "      <td>1.769</td>\n",
       "      <td>1.424</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.414</td>\n",
       "      <td>2.752</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.020</td>\n",
       "      <td>1.629</td>\n",
       "      <td>0.712</td>\n",
       "      <td>0.406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3      4      5      6\n",
       "0 -2.335 -2.330 -1.526 -0.732 -0.333 -0.158 -0.051\n",
       "1 -3.290 -4.260 -2.082 -0.855 -0.253 -0.072 -0.000\n",
       "2 -4.680  0.000 -2.648 -0.460  0.090  0.116  0.128\n",
       "3 -1.856 -1.855  1.769  1.424  0.776  0.425  0.274\n",
       "4  0.414  2.752  0.000  4.020  1.629  0.712  0.406"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "world = World(7, 5)\n",
    "world.add_terminal(1, 2, \"-\")\n",
    "world.add_terminal(2, 4, \"+\")\n",
    "\n",
    "V = np.zeros((world.width, world.height))\n",
    "returns = {(i, j): [] for i in range(world.width) for j in range(world.height)}\n",
    "\n",
    "nb_episodes = 10000\n",
    "\n",
    "for i in range(nb_episodes):\n",
    "    V, returns = mc_fv_prediction(\n",
    "        world=world,\n",
    "        start_state=(6, 4),\n",
    "        policy=equiprobable_random_policy,\n",
    "        V=V,\n",
    "        returns=returns,\n",
    "        gamma=0.9,\n",
    "    )\n",
    "display(pd.DataFrame(world.grid.T))\n",
    "display(pd.DataFrame(V.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e3d3e8",
   "metadata": {},
   "source": [
    "### Exercise: Implement Monte Carlo-based prediction for state-action values\n",
    "\n",
    "There is one more step that has to be in place before we can start to optimize a policy: estimating state-action values, $q_\\pi(s,a)$, based on experience. Above where we estimated $v_\\pi$, we only needed to keep track of the average return observed for _each state_. However, in order to estimate state-action values, we need to compute the average return observed for _each state-action_ pair.\n",
    "\n",
    "That is, for every state $(0,0), (0,1), (0,2)...$ we need to compute different estimates for the four actions ```[ \"up\", \"down\", \"left\", \"right\" ]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0a36a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_fv_state_action(world: World, start_state, policy, Q, returns, gamma=0.9):\n",
    "    episode = generate_episode(world, policy, start_state)\n",
    "\n",
    "    G = 0\n",
    "    # Iterate forward through the episode\n",
    "    for idx in reversed(range(len(episode))):\n",
    "        state, action, reward = episode[idx]\n",
    "        G = gamma * G + reward\n",
    "\n",
    "        # Check if the (state, action) pair has been seen before in the episode\n",
    "        if (state, action) not in [(s, a) for s, a, _ in episode[:idx]]:\n",
    "            returns[state][action].append(G)\n",
    "            Q[state][action] = np.mean(returns[state][action])\n",
    "\n",
    "    return Q, returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7918b63",
   "metadata": {},
   "source": [
    "Try to experiment with your implementation by running it on different world sizes (be careful not to make your world too large or you should have multiple terminals that an agent is likely to hit, otherwise it may take too long), and try to experiment with different numbers of episodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bac465f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1\n",
       "0      \n",
       "1      \n",
       "2     +"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">0</th>\n",
       "      <th colspan=\"3\" halign=\"left\">1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>up</th>\n",
       "      <td>2.947</td>\n",
       "      <td>2.918</td>\n",
       "      <td>3.902</td>\n",
       "      <td>3.250</td>\n",
       "      <td>3.220</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>down</th>\n",
       "      <td>3.932</td>\n",
       "      <td>5.715</td>\n",
       "      <td>5.639</td>\n",
       "      <td>4.993</td>\n",
       "      <td>10.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>left</th>\n",
       "      <td>2.952</td>\n",
       "      <td>3.952</td>\n",
       "      <td>5.742</td>\n",
       "      <td>2.955</td>\n",
       "      <td>3.878</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>3.270</td>\n",
       "      <td>4.885</td>\n",
       "      <td>10.000</td>\n",
       "      <td>3.224</td>\n",
       "      <td>4.974</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0                  1             \n",
       "          0     1      2     0      1     2\n",
       "up    2.947 2.918  3.902 3.250  3.220 0.000\n",
       "down  3.932 5.715  5.639 4.993 10.000 0.000\n",
       "left  2.952 3.952  5.742 2.955  3.878 0.000\n",
       "right 3.270 4.885 10.000 3.224  4.974 0.000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>down</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>down</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>right</td>\n",
       "      <td>termnal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0        1\n",
       "0   down     down\n",
       "1   down     down\n",
       "2  right  termnal"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "world = World(2, 3)\n",
    "world.add_terminal(1, 2, \"+\")\n",
    "Q = {}\n",
    "returns = {}\n",
    "for x in range(world.width):\n",
    "    for y in range(world.height):\n",
    "        Q[(x, y)] = {action: 0.0 for action in ACTIONS}\n",
    "        returns[(x, y)] = {action: [] for action in ACTIONS}\n",
    "\n",
    "nb_episodes = 10000\n",
    "for i in range(nb_episodes):\n",
    "    Q, returns = mc_fv_state_action(\n",
    "        world=world,\n",
    "        start_state=(0, 0),\n",
    "        policy=equiprobable_random_policy,\n",
    "        Q=Q,\n",
    "        returns=returns,\n",
    "        gamma=0.9,\n",
    "    )\n",
    "\n",
    "final_policy = np.full((world.width, world.height), \"          \")\n",
    "for i in range(world.width):\n",
    "    for j in range(world.height):\n",
    "        if world.is_terminal(i, j):\n",
    "            final_policy[(i, j)] = \"termnal\"\n",
    "        elif world.is_obstacle(i, j):\n",
    "            final_policy[(i, j)] = \"###\"\n",
    "        else:\n",
    "            final_policy[(i, j)] = max(Q[(i, j)], key=Q[(i, j)].get)\n",
    "\n",
    "display(pd.DataFrame(world.grid.T))\n",
    "display(pd.DataFrame(Q))\n",
    "display(pd.DataFrame(final_policy.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49031cfe",
   "metadata": {},
   "source": [
    "### Exercise: Implement on-policy Monte Carlo-based control with an $\\epsilon$-soft policy\n",
    "\n",
    "You are now ready to implement MC-based control (see page 101 of [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html) for the algorithm).\n",
    "\n",
    "In your implementation, you need to update the state-action estimates like in the exercise above, but now, you also need implement an $ϵ$-soft policy that you can modify. How could you do that?\n",
    "\n",
    "_Hint_: You can either represent your policy explicitly. That is, for each state $(x,y)$ you have a ```dict``` with actions and their probabilities which you then update each time you step through an episode. When the policy is called, it then just returns the ```dict``` with action probablities corresponding to the current state.\n",
    "\n",
    "Alternatively, you can compute the action probabilities when your policy is called based on the current action-values estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3817b40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_epsillon_soft(world, start_state, Q, policy, returns, gamma=0.9, epsilon=0.1):\n",
    "    episode = generate_episode(world, lambda x, y: policy[(x, y)], start_state)\n",
    "\n",
    "    G = 0\n",
    "    # Iterate forward through the episode\n",
    "    for idx in reversed(range(len(episode))):\n",
    "        state, action, reward = episode[idx]\n",
    "        G = gamma * G + reward\n",
    "        # Check if the (state, action) pair has been seen before in the episode\n",
    "        if (state, action) not in [(s, a) for s, a, _ in episode[:idx]]:\n",
    "            returns[state][action].append(G)\n",
    "            Q[state][action] = np.mean(returns[state][action])\n",
    "\n",
    "            # Update policy to be greedy\n",
    "            best_action = max(Q[state], key=Q[state].get)\n",
    "            for a in ACTIONS:\n",
    "                policy[state][a] = (\n",
    "                    1 - epsilon + epsilon / abs(policy[state][a])\n",
    "                    if a == best_action\n",
    "                    else epsilon / abs(policy[state][a])\n",
    "                )\n",
    "\n",
    "    return Q, policy, returns, len(episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d239f0",
   "metadata": {},
   "source": [
    "Try to experiment with your implementation by running it on different world sizes (be careful not to make your world too large or you should have multiple terminals that an agent is likely to hit, otherwise it may take too long), try to experiment with different numbers of episodes, and different values of epsilon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e27a62c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>#</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4\n",
       "0           -   \n",
       "1               \n",
       "2        #      \n",
       "3               \n",
       "4              +"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>down</td>\n",
       "      <td>down</td>\n",
       "      <td>left</td>\n",
       "      <td>termnal</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>right</td>\n",
       "      <td>down</td>\n",
       "      <td>right</td>\n",
       "      <td>down</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>right</td>\n",
       "      <td>down</td>\n",
       "      <td>###</td>\n",
       "      <td>down</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>down</td>\n",
       "      <td>right</td>\n",
       "      <td>right</td>\n",
       "      <td>right</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>right</td>\n",
       "      <td>right</td>\n",
       "      <td>right</td>\n",
       "      <td>right</td>\n",
       "      <td>termnal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2        3        4\n",
       "0   down   down   left  termnal     down\n",
       "1  right   down  right     down     down\n",
       "2  right   down    ###     down     down\n",
       "3   down  right  right    right     down\n",
       "4  right  right  right    right  termnal"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.438</td>\n",
       "      <td>1.853</td>\n",
       "      <td>1.175</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.851</td>\n",
       "      <td>2.375</td>\n",
       "      <td>3.123</td>\n",
       "      <td>5.156</td>\n",
       "      <td>5.844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.346</td>\n",
       "      <td>3.053</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.016</td>\n",
       "      <td>7.945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.800</td>\n",
       "      <td>4.846</td>\n",
       "      <td>6.165</td>\n",
       "      <td>8.002</td>\n",
       "      <td>10.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.540</td>\n",
       "      <td>4.965</td>\n",
       "      <td>6.944</td>\n",
       "      <td>10.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2      3      4\n",
       "0 1.438 1.853 1.175  0.000  0.080\n",
       "1 1.851 2.375 3.123  5.156  5.844\n",
       "2 2.346 3.053 0.000  6.016  7.945\n",
       "3 2.800 4.846 6.165  8.002 10.000\n",
       "4 3.540 4.965 6.944 10.000  0.000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "world = World(5, 5)\n",
    "world.add_terminal(4, 4, \"+\")\n",
    "world.add_terminal(3, 0, \"-\")\n",
    "world.add_obstacle(2, 2)\n",
    "n_episodes = 1000\n",
    "\n",
    "Q = {}\n",
    "policy = {}\n",
    "returns = {}\n",
    "for x in range(world.width):\n",
    "    for y in range(world.height):\n",
    "        Q[(x, y)] = {action: 0.0 for action in ACTIONS}\n",
    "        policy[(x, y)] = {action: 1 / len(ACTIONS) for action in ACTIONS}\n",
    "        returns[(x, y)] = {action: [] for action in ACTIONS}\n",
    "\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    Q, policy, returns, steps = mc_epsillon_soft(\n",
    "        world=world,\n",
    "        start_state=(0, 0),\n",
    "        Q=Q,\n",
    "        policy=policy,\n",
    "        returns=returns,\n",
    "        gamma=0.9,\n",
    "        epsilon=0.01,\n",
    "    )\n",
    "\n",
    "display(pd.DataFrame(world.grid.T))\n",
    "final_policy = np.full((world.width, world.height), \"          \")\n",
    "final_values = np.full((world.width, world.height), 0.0)\n",
    "for i in range(world.width):\n",
    "    for j in range(world.height):\n",
    "        if world.is_terminal(i, j):\n",
    "            final_policy[(i, j)] = \"termnal\"\n",
    "        elif world.is_obstacle(i, j):\n",
    "            final_policy[(i, j)] = \"###\"\n",
    "        else:\n",
    "            final_policy[(i, j)] = max(Q[(i, j)], key=Q[(i, j)].get)\n",
    "            final_values[(i, j)] = max(Q[(i, j)].items(), key=lambda x: x[1])[1]\n",
    "\n",
    "\n",
    "display(pd.DataFrame(final_policy.T))\n",
    "display(pd.DataFrame(final_values.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773fb8bb",
   "metadata": {},
   "source": [
    "### Optional exercise\n",
    "\n",
    "Try to implement exploring starts (see page 99 of [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html) for the algorithm). It should be straightforward and only require minimal changes to the code for the exercise above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37104b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_exploring_starts(world, Q, policy, returns, gamma=0.9):\n",
    "    # Exploring start - random initial state and action\n",
    "    start_state = (\n",
    "        random.randint(0, world.width - 1),\n",
    "        random.randint(0, world.height - 1),\n",
    "    )\n",
    "    # Do not start in a terminal state or an obstacle\n",
    "    while world.is_terminal(*start_state) or world.is_obstacle(*start_state):\n",
    "        start_state = (\n",
    "            random.randint(0, world.width - 1),\n",
    "            random.randint(0, world.height - 1),\n",
    "        )\n",
    "\n",
    "    episode = generate_episode(world, lambda x, y: policy[(x, y)], start_state)\n",
    "\n",
    "    G = 0\n",
    "    # Iterate forward through the episode\n",
    "    for idx in reversed(range(len(episode))):\n",
    "        state, action, reward = episode[idx]\n",
    "        # print(f\"State: {state}, Action: {action}, Reward: {reward}\")\n",
    "        G = gamma * G + reward\n",
    "        # Check if the (state, action) pair has been seen before in the episode\n",
    "        if (state, action) not in [(s, a) for s, a, _ in episode[:idx]]:\n",
    "            returns[state][action].append(G)\n",
    "            Q[state][action] = np.mean(returns[state][action])\n",
    "\n",
    "            # Update policy to be greedy\n",
    "            _, max_value = max(Q[state].items(), key=lambda x: x[1])\n",
    "            # Count cells that have the same max value\n",
    "            max_value_count = sum(Q[state][action] == max_value for action in ACTIONS)\n",
    "            for action in ACTIONS:\n",
    "                if Q[state][action] == max_value:\n",
    "                    policy[state][action] = 1.0 / max_value_count\n",
    "                else:\n",
    "                    policy[state][action] = 0.0\n",
    "\n",
    "    return Q, policy, returns, len(episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ea54c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                       | 3/1000 [00:27<2:31:56,  9.14s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m         returns[(x, y)] \u001b[38;5;241m=\u001b[39m {action: [] \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m ACTIONS}\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(n_episodes)):\n\u001b[0;32m---> 18\u001b[0m     Q, policy, returns, steps \u001b[38;5;241m=\u001b[39m \u001b[43mmc_exploring_starts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworld\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworld\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mQ\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m display(pd\u001b[38;5;241m.\u001b[39mDataFrame(world\u001b[38;5;241m.\u001b[39mgrid\u001b[38;5;241m.\u001b[39mT))\n\u001b[1;32m     26\u001b[0m final_policy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfull((world\u001b[38;5;241m.\u001b[39mwidth, world\u001b[38;5;241m.\u001b[39mheight), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m          \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 14\u001b[0m, in \u001b[0;36mmc_exploring_starts\u001b[0;34m(world, Q, policy, returns, gamma)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m world\u001b[38;5;241m.\u001b[39mis_terminal(\u001b[38;5;241m*\u001b[39mstart_state) \u001b[38;5;129;01mor\u001b[39;00m world\u001b[38;5;241m.\u001b[39mis_obstacle(\u001b[38;5;241m*\u001b[39mstart_state):\n\u001b[1;32m      9\u001b[0m     start_state \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     10\u001b[0m         random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, world\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     11\u001b[0m         random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, world\u001b[38;5;241m.\u001b[39mheight \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     12\u001b[0m     )\n\u001b[0;32m---> 14\u001b[0m episode \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m G \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Iterate forward through the episode\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m, in \u001b[0;36mgenerate_episode\u001b[0;34m(world, policy, start_state)\u001b[0m\n\u001b[1;32m      7\u001b[0m possible_actions \u001b[38;5;241m=\u001b[39m policy(\u001b[38;5;241m*\u001b[39mcurrent_state)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Pick a weighted random action:\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoices\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpopulation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpossible_actions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpossible_actions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Get the next state from the world\u001b[39;00m\n\u001b[1;32m     17\u001b[0m next_state \u001b[38;5;241m=\u001b[39m world\u001b[38;5;241m.\u001b[39mget_next_state(current_state, action[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/random.py:498\u001b[0m, in \u001b[0;36mRandom.choices\u001b[0;34m(self, population, weights, cum_weights, k)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot specify both weights and cumulative weights\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcum_weights\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m:\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe number of weights does not match the population\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    500\u001b[0m total \u001b[38;5;241m=\u001b[39m cum_weights[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.0\u001b[39m   \u001b[38;5;66;03m# convert to float\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "world = World(5, 5)\n",
    "world.add_terminal(4, 4, \"+\")\n",
    "world.add_terminal(3, 0, \"-\")\n",
    "world.add_obstacle(2, 2)\n",
    "n_episodes = 1000\n",
    "\n",
    "Q = {}\n",
    "policy = {}\n",
    "returns = {}\n",
    "for x in range(world.width):\n",
    "    for y in range(world.height):\n",
    "        Q[(x, y)] = {action: 0.0 for action in ACTIONS}\n",
    "        policy[(x, y)] = {action: 1 / len(ACTIONS) for action in ACTIONS}\n",
    "        returns[(x, y)] = {action: [] for action in ACTIONS}\n",
    "\n",
    "\n",
    "for i in tqdm(range(n_episodes)):\n",
    "    Q, policy, returns, steps = mc_exploring_starts(\n",
    "        world=world,\n",
    "        Q=Q,\n",
    "        policy=policy,\n",
    "        returns=returns,\n",
    "        gamma=0.9,\n",
    "    )\n",
    "display(pd.DataFrame(world.grid.T))\n",
    "final_policy = np.full((world.width, world.height), \"          \")\n",
    "final_values = np.full((world.width, world.height), 0.0)\n",
    "for i in range(world.width):\n",
    "    for j in range(world.height):\n",
    "        if world.is_terminal(i, j):\n",
    "            final_policy[(i, j)] = \"termnal\"\n",
    "        elif world.is_obstacle(i, j):\n",
    "            final_policy[(i, j)] = \"###\"\n",
    "        else:\n",
    "            final_policy[(i, j)] = max(Q[(i, j)], key=Q[(i, j)].get)\n",
    "            final_values[(i, j)] = max(Q[(i, j)].items(), key=lambda x: x[1])[1]\n",
    "display(pd.DataFrame(final_policy.T))\n",
    "display(pd.DataFrame(final_values.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916ec073",
   "metadata": {},
   "source": [
    "# Soft epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1df82fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "world = World(5, 5)\n",
    "world.add_terminal(3, 3, \"+\")\n",
    "n_episodes = 500\n",
    "epsilon_001 = 0.01\n",
    "epsilon_005 = 0.05\n",
    "epsilon_01 = 0.1\n",
    "instances = 1000\n",
    "\n",
    "steps_total_001 = [0] * n_episodes\n",
    "steps_total_01 = [0] * n_episodes\n",
    "steps_total_005 = [0] * n_episodes\n",
    "for i in tqdm(range(instances)):\n",
    "    Q = {}\n",
    "    policy = {}\n",
    "    returns = {}\n",
    "    steps_temp = []\n",
    "    for x in range(world.width):\n",
    "        for y in range(world.height):\n",
    "            Q[(x, y)] = {action: 0.0 for action in ACTIONS}\n",
    "            policy[(x, y)] = {action: 1 / len(ACTIONS) for action in ACTIONS}\n",
    "            returns[(x, y)] = {action: [] for action in ACTIONS}\n",
    "\n",
    "    for i in range(n_episodes):\n",
    "        Q, policy, returns, steps = mc_epsillon_soft(\n",
    "            world=world,\n",
    "            start_state=(0, 0),\n",
    "            Q=Q,\n",
    "            policy=policy,\n",
    "            returns=returns,\n",
    "            gamma=0.9,\n",
    "            epsilon=epsilon_001,\n",
    "        )\n",
    "        steps_temp.append(steps)\n",
    "    steps_total_001 = [sum(values) for values in zip(steps_total_001, steps_temp)]\n",
    "\n",
    "for i in tqdm(range(instances)):\n",
    "    Q = {}\n",
    "    policy = {}\n",
    "    returns = {}\n",
    "    steps_temp = []\n",
    "    for x in range(world.width):\n",
    "        for y in range(world.height):\n",
    "            Q[(x, y)] = {action: 0.0 for action in ACTIONS}\n",
    "            policy[(x, y)] = {action: 1 / len(ACTIONS) for action in ACTIONS}\n",
    "            returns[(x, y)] = {action: [] for action in ACTIONS}\n",
    "\n",
    "    for i in range(n_episodes):\n",
    "        Q, policy, returns, steps = mc_epsillon_soft(\n",
    "            world=world,\n",
    "            start_state=(0, 0),\n",
    "            Q=Q,\n",
    "            policy=policy,\n",
    "            returns=returns,\n",
    "            gamma=0.9,\n",
    "            epsilon=epsilon_01,\n",
    "        )\n",
    "        steps_temp.append(steps)\n",
    "    steps_total_01 = [sum(values) for values in zip(steps_total_01, steps_temp)]\n",
    "\n",
    "for i in tqdm(range(instances)):\n",
    "    Q = {}\n",
    "    policy = {}\n",
    "    returns = {}\n",
    "    steps_temp = []\n",
    "    for x in range(world.width):\n",
    "        for y in range(world.height):\n",
    "            Q[(x, y)] = {action: 0.0 for action in ACTIONS}\n",
    "            policy[(x, y)] = {action: 1 / len(ACTIONS) for action in ACTIONS}\n",
    "            returns[(x, y)] = {action: [] for action in ACTIONS}\n",
    "\n",
    "    for i in range(n_episodes):\n",
    "        Q, policy, returns, steps = mc_epsillon_soft(\n",
    "            world=world,\n",
    "            start_state=(0, 0),\n",
    "            Q=Q,\n",
    "            policy=policy,\n",
    "            returns=returns,\n",
    "            gamma=0.9,\n",
    "            epsilon=epsilon_005,\n",
    "        )\n",
    "        steps_temp.append(steps)\n",
    "    steps_total_005 = [sum(values) for values in zip(steps_total_005, steps_temp)]\n",
    "steps_total_001 = [value / instances for value in steps_total_001]\n",
    "steps_total_005 = [value / instances for value in steps_total_005]\n",
    "steps_total_01 = [value / instances for value in steps_total_01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fd467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6), dpi=120)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "plt.ylim(0, 100)\n",
    "plt.xlim(0, n_episodes)\n",
    "\n",
    "# Plot the\n",
    "ax.plot(steps_total_001, \"r\", markersize=1)\n",
    "ax.plot(steps_total_005, \"g\", markersize=1)\n",
    "ax.plot(steps_total_01, \"y\", markersize=1)\n",
    "\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Steps\")\n",
    "plt.legend(\n",
    "    [\n",
    "        \"Epsilon 0.01\",\n",
    "        \"Epsilon 0.05\",\n",
    "        \"Epsilon 0.1\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "plt.title(\n",
    "    f\"Monte Carlo-based control with an epsilon-soft policy for {instances} instances.\"\n",
    ")\n",
    "\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db181036",
   "metadata": {},
   "source": [
    "# Soft epsilon vs ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd126ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "world = World(4, 4)\n",
    "world.add_terminal(3, 3, \"+\")\n",
    "n_episodes = 500\n",
    "instances = 150\n",
    "\n",
    "steps_total_ES = [0] * n_episodes\n",
    "steps_total_MC = [0] * n_episodes\n",
    "\n",
    "for i in tqdm(range(instances)):\n",
    "    Q = {}\n",
    "    policy = {}\n",
    "    returns = {}\n",
    "    steps_temp = []\n",
    "    for x in range(world.width):\n",
    "        for y in range(world.height):\n",
    "            Q[(x, y)] = {action: 0.0 for action in ACTIONS}\n",
    "            policy[(x, y)] = {action: 1 / len(ACTIONS) for action in ACTIONS}\n",
    "            returns[(x, y)] = {action: [] for action in ACTIONS}\n",
    "\n",
    "    for i in range(n_episodes):\n",
    "        Q, policy, returns, steps = mc_exploring_starts(\n",
    "            world=world,\n",
    "            Q=Q,\n",
    "            policy=policy,\n",
    "            returns=returns,\n",
    "            gamma=0.9,\n",
    "        )\n",
    "        steps_temp.append(steps)\n",
    "    steps_total_ES = [sum(values) for values in zip(steps_total_ES, steps_temp)]\n",
    "\n",
    "for i in tqdm(range(instances)):\n",
    "    Q = {}\n",
    "    policy = {}\n",
    "    returns = {}\n",
    "    steps_temp = []\n",
    "    for x in range(world.width):\n",
    "        for y in range(world.height):\n",
    "            Q[(x, y)] = {action: 0.0 for action in ACTIONS}\n",
    "            policy[(x, y)] = {action: 1 / len(ACTIONS) for action in ACTIONS}\n",
    "            returns[(x, y)] = {action: [] for action in ACTIONS}\n",
    "\n",
    "    for i in range(n_episodes):\n",
    "        Q, policy, returns, steps = mc_epsillon_soft(\n",
    "            world=world,\n",
    "            start_state=(0, 0),\n",
    "            Q=Q,\n",
    "            policy=policy,\n",
    "            returns=returns,\n",
    "            gamma=0.9,\n",
    "            epsilon=0.01,\n",
    "        )\n",
    "        steps_temp.append(steps)\n",
    "    steps_total_MC = [sum(values) for values in zip(steps_total_MC, steps_temp)]\n",
    "\n",
    "\n",
    "steps_total_ES = [value / instances for value in steps_total_ES]\n",
    "steps_total_MC = [value / instances for value in steps_total_MC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e70c5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6), dpi=120)\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "plt.ylim(0, 100)\n",
    "plt.xlim(0, n_episodes)\n",
    "\n",
    "# Plot the\n",
    "ax.plot(steps_total_ES, \"r\", markersize=1)\n",
    "ax.plot(steps_total_MC, \"b\", markersize=1)\n",
    "\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Steps\")\n",
    "plt.legend(\n",
    "    [\n",
    "        \"ES\",\n",
    "        \"MC\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "plt.title(f\"MC epsilon-soft policy vs MC Exploring Starts for {instances} instances.\")\n",
    "\n",
    "plt.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
