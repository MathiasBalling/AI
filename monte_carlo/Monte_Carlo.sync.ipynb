{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bee904f5",
   "metadata": {},
   "source": [
    "# Monte Carlo approaches to prediction and control\n",
    "\n",
    "In this notebook, you will implement the Monte Carlo approaches to prediction and control described in [Sutton and Barto's book, Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html). We will use the grid ```World``` class from the previous lecture, but now without relying on knowledge of the task dynamics, that is, without relying on knowledge about transition probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b490c0a",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d851c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/balling/projects/AI/env/lib/python3.9/site-packages (2.0.2)\n",
      "Requirement already satisfied: pandas in /Users/balling/projects/AI/env/lib/python3.9/site-packages (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/balling/projects/AI/env/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/balling/projects/AI/env/lib/python3.9/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/balling/projects/AI/env/lib/python3.9/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/balling/projects/AI/env/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install numpy pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28d588e",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d53ac59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys  # We use sys to get the max value of a float\n",
    "import pandas as pd  # We only use pandas for displaying tables nicely\n",
    "from IPython.display import display\n",
    "from pandas.core.arrays.arrow.accessors import ListAccessor\n",
    "\n",
    "pd.options.display.float_format = \"{:,.3f}\".format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954f35da",
   "metadata": {},
   "source": [
    "### ```World``` class and globals\n",
    "\n",
    "The ```World``` is a grid represented as a two-dimensional array of characters where each character can represent free space, an obstacle, or a terminal. Each non-obstacle cell is associated with a reward that an agent gets for moving to that cell (can be 0). The size of the world is _width_ $\\times$ _height_ characters.\n",
    "\n",
    "A _state_ is a tuple $(x,y)$.\n",
    "\n",
    "An empty world is created in the ```__init__``` method. Obstacles, rewards and terminals can then be added with ```add_obstacle``` and ```add_reward```.\n",
    "\n",
    "To calculate the next state of an agent (that is, an agent is in some state $s = (x,y)$ and performs and action, $a$), ```get_next_state()```should be called.\n",
    "\n",
    "__Note that ```get_state_transition_probabilities``` has been removed and an agent must now rely on experience interacting with a world to learn.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2916bff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals:\n",
    "ACTIONS = (\"up\", \"down\", \"left\", \"right\")\n",
    "\n",
    "# Rewards, terminals and obstacles are characters:\n",
    "REWARDS = {\" \": 0, \".\": 0.1, \"+\": 10, \"-\": -10}\n",
    "TERMINALS = (\"+\", \"-\")  # Note a terminal should also have a reward assigned\n",
    "OBSTACLES = \"#\"\n",
    "\n",
    "# Discount factor\n",
    "gamma = 1\n",
    "\n",
    "# The probability of a random move:\n",
    "rand_move_probability = 0\n",
    "\n",
    "\n",
    "class World:\n",
    "    def __init__(self, width, height):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        # Create an empty world where the agent can move to all cells\n",
    "        self.grid = np.full((width, height), \" \", dtype=\"U1\")\n",
    "\n",
    "    def add_obstacle(self, start_x, start_y, end_x=None, end_y=None):\n",
    "        \"\"\"\n",
    "        Create an obstacle in either a single cell or rectangle.\n",
    "        \"\"\"\n",
    "        if end_x == None:\n",
    "            end_x = start_x\n",
    "        if end_y == None:\n",
    "            end_y = start_y\n",
    "\n",
    "        self.grid[start_x : end_x + 1, start_y : end_y + 1] = OBSTACLES[0]\n",
    "\n",
    "    def add_reward(self, x, y, reward):\n",
    "        assert reward in REWARDS, f\"{reward} not in {REWARDS}\"\n",
    "        self.grid[x, y] = reward\n",
    "\n",
    "    def add_terminal(self, x, y, terminal):\n",
    "        assert terminal in TERMINALS, f\"{terminal} not in {TERMINALS}\"\n",
    "        self.grid[x, y] = terminal\n",
    "\n",
    "    def is_obstacle(self, x, y):\n",
    "        if x < 0 or x >= self.width or y < 0 or y >= self.height:\n",
    "            return True\n",
    "        else:\n",
    "            return self.grid[x, y] in OBSTACLES\n",
    "\n",
    "    def is_terminal(self, x, y):\n",
    "        return self.grid[x, y] in TERMINALS\n",
    "\n",
    "    def get_reward(self, x, y):\n",
    "        \"\"\"\n",
    "        Return the reward associated with a given location\n",
    "        \"\"\"\n",
    "        return REWARDS[self.grid[x, y]]\n",
    "\n",
    "    def get_next_state(self, current_state, action):\n",
    "        \"\"\"\n",
    "        Get the next state given a current state and an action. The outcome can be\n",
    "        stochastic  where rand_move_probability determines the probability of\n",
    "        ignoring the action and performing a random move.\n",
    "        \"\"\"\n",
    "        assert action in ACTIONS, f\"Unknown acion {action} must be one of {ACTIONS}\"\n",
    "\n",
    "        x, y = current_state\n",
    "\n",
    "        # If our current state is a terminal, there is no next state\n",
    "        if self.grid[x, y] in TERMINALS:\n",
    "            return None\n",
    "\n",
    "        # Check of a random action should be performed:\n",
    "        if np.random.rand() < rand_move_probability:\n",
    "            action = np.random.choice(ACTIONS)\n",
    "\n",
    "        if action == \"up\":\n",
    "            y -= 1\n",
    "        elif action == \"down\":\n",
    "            y += 1\n",
    "        elif action == \"left\":\n",
    "            x -= 1\n",
    "        elif action == \"right\":\n",
    "            x += 1\n",
    "\n",
    "        # If the next state is an obstacle, stay in the current state\n",
    "        return (x, y) if not self.is_obstacle(x, y) else current_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c26e2e",
   "metadata": {},
   "source": [
    "## Basic example: Generating episodes\n",
    "\n",
    "An episode is the series of states, actions and rewards reflecting an agent's experience interacting with the environment. An episode starts with an agent being placed at some initial state and continues till the agent reaches a terminal state.  To generate episodes, we first need a world and a policy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c978014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ']\n",
      " [' ' ' ']\n",
      " [' ' '+']]\n"
     ]
    }
   ],
   "source": [
    "world = World(2, 3)\n",
    "\n",
    "# Since we only focus on episodic tasks, we must have a terminal state that the\n",
    "# agent eventually reaches\n",
    "world.add_terminal(1, 2, \"+\")\n",
    "\n",
    "\n",
    "def equiprobable_random_policy(x, y):\n",
    "    return {k: 1 / len(ACTIONS) for k in ACTIONS}\n",
    "\n",
    "\n",
    "print(world.grid.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abad9deb",
   "metadata": {},
   "source": [
    "To generate an episode, we need to provide a ```World```, a policy, and a start state.\n",
    "\n",
    "In each step, we do the following:\n",
    "1. perform one of the actions (weighted random) returned by the policy for the giving state\n",
    "2. get the reward and add a new entry to the episode $[S_t, A_t, R_{t+1}]$\n",
    "3. move the agent to the next state\n",
    "\n",
    "When a terminal state is reached, we return all the $[[S_0, A_0, R_1], ..., [S_{T}, A_T, R_{T+1}]]$ observed in the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a599f403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(world: World, policy, start_state):\n",
    "    current_state = start_state\n",
    "    episode = []\n",
    "    while not world.is_terminal(*current_state):\n",
    "        # Get the possible actions and their probabilities that our policy says\n",
    "        # that the agent should perform in the current state:\n",
    "        possible_actions = policy(*current_state)\n",
    "\n",
    "        # Pick a weighted random action:\n",
    "        action = random.choices(\n",
    "            population=list(possible_actions.keys()),\n",
    "            weights=possible_actions.values(),\n",
    "            k=1,\n",
    "        )\n",
    "\n",
    "        # Get the next state from the world\n",
    "        next_state = world.get_next_state(current_state, action[0])\n",
    "\n",
    "        # Get the reward for performing the action\n",
    "        reward = world.get_reward(*next_state)\n",
    "\n",
    "        # Save the state, action and reward for this time step in our episode\n",
    "        episode.append([current_state, action[0], reward])\n",
    "\n",
    "        # Move the agent to the new state\n",
    "        current_state = next_state\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b08703",
   "metadata": {},
   "source": [
    "Now, we can try to generate a couple of episodes and print the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdb7cd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0:\n",
      "    State Action  Reward\n",
      "0  (0, 0)   down       0\n",
      "1  (0, 1)   down       0\n",
      "2  (0, 2)  right      10\n",
      "\n",
      "Episode 1:\n",
      "    State Action  Reward\n",
      "0  (0, 0)   left       0\n",
      "1  (0, 0)   down       0\n",
      "2  (0, 1)   down       0\n",
      "3  (0, 2)   down       0\n",
      "4  (0, 2)   down       0\n",
      "5  (0, 2)     up       0\n",
      "6  (0, 1)   left       0\n",
      "7  (0, 1)   down       0\n",
      "8  (0, 2)  right      10\n",
      "\n",
      "Episode 2:\n",
      "     State Action  Reward\n",
      "0   (0, 0)   down       0\n",
      "1   (0, 1)     up       0\n",
      "2   (0, 0)  right       0\n",
      "3   (1, 0)     up       0\n",
      "4   (1, 0)  right       0\n",
      "5   (1, 0)   down       0\n",
      "6   (1, 1)  right       0\n",
      "7   (1, 1)  right       0\n",
      "8   (1, 1)   left       0\n",
      "9   (0, 1)   down       0\n",
      "10  (0, 2)   left       0\n",
      "11  (0, 2)  right      10\n",
      "\n",
      "Episode 3:\n",
      "     State Action  Reward\n",
      "0   (0, 0)   down       0\n",
      "1   (0, 1)   left       0\n",
      "2   (0, 1)   down       0\n",
      "3   (0, 2)   left       0\n",
      "4   (0, 2)     up       0\n",
      "5   (0, 1)   left       0\n",
      "6   (0, 1)     up       0\n",
      "7   (0, 0)   down       0\n",
      "8   (0, 1)   down       0\n",
      "9   (0, 2)     up       0\n",
      "10  (0, 1)  right       0\n",
      "11  (1, 1)     up       0\n",
      "12  (1, 0)   left       0\n",
      "13  (0, 0)   down       0\n",
      "14  (0, 1)  right       0\n",
      "15  (1, 1)     up       0\n",
      "16  (1, 0)   left       0\n",
      "17  (0, 0)  right       0\n",
      "18  (1, 0)   down       0\n",
      "19  (1, 1)  right       0\n",
      "20  (1, 1)   down      10\n",
      "\n",
      "Episode 4:\n",
      "     State Action  Reward\n",
      "0   (0, 0)   down       0\n",
      "1   (0, 1)   down       0\n",
      "2   (0, 2)   left       0\n",
      "3   (0, 2)   left       0\n",
      "4   (0, 2)   down       0\n",
      "5   (0, 2)   down       0\n",
      "6   (0, 2)   down       0\n",
      "7   (0, 2)     up       0\n",
      "8   (0, 1)   left       0\n",
      "9   (0, 1)     up       0\n",
      "10  (0, 0)  right       0\n",
      "11  (1, 0)     up       0\n",
      "12  (1, 0)  right       0\n",
      "13  (1, 0)     up       0\n",
      "14  (1, 0)   down       0\n",
      "15  (1, 1)   down      10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Episode {i}:\")\n",
    "    episode = generate_episode(world, equiprobable_random_policy, (0, 0))\n",
    "    print(pd.DataFrame(episode, columns=[\"State\", \"Action\", \"Reward\"]), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19535fa4",
   "metadata": {},
   "source": [
    "### Exercise: Implement Monte Carlo-based prediction for state values\n",
    "\n",
    "You should implement first-visit MC prediction for estimating $V≈v_\\pi$. See page 92 of [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8d4ad36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_fv_prediction(world: World, policy, nb_episodes, gamma):\n",
    "    V = np.zeros((world.width, world.height))\n",
    "    Returns = {}\n",
    "    # Initialize returns as lists\n",
    "    for i in range(world.width):\n",
    "        for j in range(world.height):\n",
    "            Returns[i, j] = []\n",
    "\n",
    "    for _ in range(nb_episodes):\n",
    "        # Generate a random start state\n",
    "        start_state = (np.random.randint(world.width), np.random.randint(world.height))\n",
    "        episode = generate_episode(world, policy, start_state)\n",
    "        G = 0\n",
    "        for t, (state, _, reward) in enumerate(reversed(episode)):\n",
    "            idx = len(episode) - t - 1\n",
    "            G = gamma * G + reward\n",
    "            if state not in [s for s, _, _ in episode[:idx]]:\n",
    "                Returns[state].append(G)\n",
    "                V[state] = np.average(Returns[state])\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c6e537",
   "metadata": {},
   "source": [
    "First, try your algorithm on the small $2\\times3$ world above using an equiprobable policy and $\\gamma = 0.9$. Depending on the number of episodes you use, you should get close to the true values:\n",
    "\n",
    "<table class=\"dataframe\" border=\"1\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>0</th>\n",
    "      <th>1</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>3.283</td>\n",
    "      <td>3.616</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>4.409</td>\n",
    "      <td>5.556</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>6.349</td>\n",
    "      <td>0.000</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f3535e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.273</td>\n",
       "      <td>3.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.374</td>\n",
       "      <td>5.562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.294</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0 3.273 3.600\n",
       "1 4.374 5.562\n",
       "2 6.294 0.000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "world = World(2, 3)\n",
    "world.add_terminal(1, 2, \"+\")\n",
    "V = mc_fv_prediction(world, equiprobable_random_policy, 10000, gamma)\n",
    "display(pd.DataFrame(V.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a0ef10",
   "metadata": {},
   "source": [
    "Try to run your MC prediction code on worlds of different sizes (be careful not to make your world too large or you should have multiple terminals that an agent is likely to hit, otherwise it may take too long). You can try to change the policy as well, but rememeber that the agent **must** eventually reach a terminal state under any policy that you try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efe0bc29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>+</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>+</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5  6\n",
       "0                     \n",
       "1                     \n",
       "2     +               \n",
       "3                     \n",
       "4        +            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.632</td>\n",
       "      <td>2.693</td>\n",
       "      <td>2.101</td>\n",
       "      <td>1.422</td>\n",
       "      <td>0.947</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.661</td>\n",
       "      <td>4.637</td>\n",
       "      <td>3.104</td>\n",
       "      <td>1.895</td>\n",
       "      <td>1.147</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.896</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.159</td>\n",
       "      <td>2.688</td>\n",
       "      <td>1.461</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.916</td>\n",
       "      <td>6.330</td>\n",
       "      <td>5.882</td>\n",
       "      <td>3.456</td>\n",
       "      <td>1.817</td>\n",
       "      <td>1.007</td>\n",
       "      <td>0.704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.612</td>\n",
       "      <td>6.423</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.997</td>\n",
       "      <td>2.343</td>\n",
       "      <td>1.211</td>\n",
       "      <td>0.776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2     3     4     5     6\n",
       "0 2.632 2.693 2.101 1.422 0.947 0.619 0.457\n",
       "1 3.661 4.637 3.104 1.895 1.147 0.700 0.509\n",
       "2 5.896 0.000 5.159 2.688 1.461 0.823 0.591\n",
       "3 4.916 6.330 5.882 3.456 1.817 1.007 0.704\n",
       "4 4.612 6.423 0.000 4.997 2.343 1.211 0.776"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "world = World(7, 5)\n",
    "world.add_terminal(1, 2, \"+\")\n",
    "world.add_terminal(2, 4, \"+\")\n",
    "V = mc_fv_prediction(world, equiprobable_random_policy, 10000, gamma)\n",
    "display(pd.DataFrame(world.grid.T))\n",
    "display(pd.DataFrame(V.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e3d3e8",
   "metadata": {},
   "source": [
    "### Exercise: Implement Monte Carlo-based prediction for state-action values\n",
    "\n",
    "There is one more step that has to be in place before we can start to optimize a policy: estimating state-action values, $q_\\pi(s,a)$, based on experience. Above where we estimated $v_\\pi$, we only needed to keep track of the average return observed for _each state_. However, in order to estimate state-action values, we need to compute the average return observed for _each state-action_ pair.\n",
    "\n",
    "That is, for every state $(0,0), (0,1), (0,2)...$ we need to compute different estimates for the four actions ```[ \"up\", \"down\", \"left\", \"right\" ]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0a36a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_fv_state_action(world, policy, nb_episodes, gamma=0.9):\n",
    "    Q = {}\n",
    "    returns = {}\n",
    "\n",
    "    # Initialize Q, policy, and returns\n",
    "    for i in range(world.width):\n",
    "        for j in range(world.height):\n",
    "            Q[(i, j)] = {action: 0.0 for action in ACTIONS}\n",
    "            returns[(i, j)] = {action: [] for action in ACTIONS}\n",
    "\n",
    "    for _ in range(nb_episodes):\n",
    "        # Exploring start - random initial state and action\n",
    "        start_state = (0, 0)\n",
    "        episode = generate_episode(world, policy, start_state)\n",
    "\n",
    "        G = 0\n",
    "        # Iterate forward through the episode\n",
    "        for t, (state, action, reward) in enumerate(reversed(episode)):\n",
    "            idx = len(episode) - t - 1\n",
    "            G = gamma * G + reward\n",
    "\n",
    "            # Check if the (state, action) pair has been seen before in the episode\n",
    "            if (state, action) not in [(s, a) for s, a, _ in episode[:idx]]:\n",
    "                returns[state][action].append(G)\n",
    "                Q[state][action] = np.mean(returns[state][action])\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7918b63",
   "metadata": {},
   "source": [
    "Try to experiment with your implementation by running it on different world sizes (be careful not to make your world too large or you should have multiple terminals that an agent is likely to hit, otherwise it may take too long), and try to experiment with different numbers of episodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bac465f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">0</th>\n",
       "      <th colspan=\"3\" halign=\"left\">1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>up</th>\n",
       "      <td>3.013</td>\n",
       "      <td>2.937</td>\n",
       "      <td>4.013</td>\n",
       "      <td>3.267</td>\n",
       "      <td>3.288</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>down</th>\n",
       "      <td>4.006</td>\n",
       "      <td>5.783</td>\n",
       "      <td>5.800</td>\n",
       "      <td>4.971</td>\n",
       "      <td>10.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>left</th>\n",
       "      <td>2.981</td>\n",
       "      <td>3.927</td>\n",
       "      <td>5.886</td>\n",
       "      <td>2.963</td>\n",
       "      <td>3.960</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right</th>\n",
       "      <td>3.258</td>\n",
       "      <td>4.981</td>\n",
       "      <td>10.000</td>\n",
       "      <td>3.323</td>\n",
       "      <td>5.028</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0                  1             \n",
       "          0     1      2     0      1     2\n",
       "up    3.013 2.937  4.013 3.267  3.288 0.000\n",
       "down  4.006 5.783  5.800 4.971 10.000 0.000\n",
       "left  2.981 3.927  5.886 2.963  3.960 0.000\n",
       "right 3.258 4.981 10.000 3.323  5.028 0.000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "world = World(2, 3)\n",
    "world.add_terminal(1, 2, \"+\")\n",
    "Q = mc_fv_state_action(world, equiprobable_random_policy, 10000, gamma)\n",
    "display(pd.DataFrame(Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49031cfe",
   "metadata": {},
   "source": [
    "### Exercise: Implement on-policy Monte Carlo-based control with an $\\epsilon$-soft policy\n",
    "\n",
    "You are now ready to implement MC-based control (see page 101 of [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html) for the algorithm).\n",
    "\n",
    "In your implementation, you need to update the state-action estimates like in the exercise above, but now, you also need implement an $ϵ$-soft policy that you can modify. How could you do that?\n",
    "\n",
    "_Hint_: You can either represent your policy explicitly. That is, for each state $(x,y)$ you have a ```dict``` with actions and their probabilities which you then update each time you step through an episode. When the policy is called, it then just returns the ```dict``` with action probablities corresponding to the current state.\n",
    "\n",
    "Alternatively, you can compute the action probabilities when your policy is called based on the current action-values estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3817b40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_epsillon_soft(world, nb_episodes, gamma=0.9, epsilon=0.1):\n",
    "    policy = {}\n",
    "    Q = {}\n",
    "    returns = {}\n",
    "\n",
    "    # Initialize Q, policy, and returns\n",
    "    for i in range(world.width):\n",
    "        for j in range(world.height):\n",
    "            Q[(i, j)] = {action: 0.0 for action in ACTIONS}\n",
    "            policy[(i, j)] = {action: 1 / len(ACTIONS) for action in ACTIONS}\n",
    "            returns[(i, j)] = {action: [] for action in ACTIONS}\n",
    "\n",
    "    for _ in range(nb_episodes):\n",
    "        start_state = (0, 0)\n",
    "        episode = generate_episode(world, lambda x, y: policy[(x, y)], start_state)\n",
    "\n",
    "        G = 0\n",
    "        # Iterate forward through the episode\n",
    "        for t, (state, action, reward) in enumerate(reversed(episode)):\n",
    "            idx = len(episode) - t - 1\n",
    "            G = gamma * G + reward\n",
    "            # Check if the (state, action) pair has been seen before in the episode\n",
    "            if (state, action) not in [(s, a) for s, a, _ in episode[:idx]]:\n",
    "                returns[state][action].append(G)\n",
    "                Q[state][action] = np.mean(returns[state][action])\n",
    "\n",
    "                # Update policy to be greedy\n",
    "                best_action = max(Q[state], key=Q[state].get)\n",
    "                for a in ACTIONS:\n",
    "                    policy[state][a] = (\n",
    "                        1 - epsilon + epsilon / abs(policy[state][a])\n",
    "                        if a == best_action\n",
    "                        else epsilon / abs(policy[state][a])\n",
    "                    )\n",
    "\n",
    "    return Q, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d239f0",
   "metadata": {},
   "source": [
    "Try to experiment with your implementation by running it on different world sizes (be careful not to make your world too large or you should have multiple terminals that an agent is likely to hit, otherwise it may take too long), try to experiment with different numbers of episodes, and different values of epsilon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e27a62c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>+</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2\n",
       "0  +      \n",
       "1         \n",
       "2         "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>up</th>\n",
       "      <th>down</th>\n",
       "      <th>left</th>\n",
       "      <th>right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2</th>\n",
       "      <th>0</th>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       up  down  left  right\n",
       "0 0 0.250 0.250 0.250  0.250\n",
       "  1 0.250 0.250 0.250  0.250\n",
       "  2 0.250 0.250 0.250  0.250\n",
       "1 0 0.250 0.250 0.250  0.250\n",
       "  1 0.250 0.250 0.250  0.250\n",
       "  2 0.250 0.250 0.250  0.250\n",
       "2 0 0.250 0.250 0.250  0.250\n",
       "  1 0.250 0.250 0.250  0.250\n",
       "  2 0.250 0.250 0.250  0.250"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>up</th>\n",
       "      <th>down</th>\n",
       "      <th>left</th>\n",
       "      <th>right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2</th>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       up  down  left  right\n",
       "0 0 0.000 0.000 0.000  0.000\n",
       "  1 0.000 0.000 0.000  0.000\n",
       "  2 0.000 0.000 0.000  0.000\n",
       "1 0 0.000 0.000 0.000  0.000\n",
       "  1 0.000 0.000 0.000  0.000\n",
       "  2 0.000 0.000 0.000  0.000\n",
       "2 0 0.000 0.000 0.000  0.000\n",
       "  1 0.000 0.000 0.000  0.000\n",
       "  2 0.000 0.000 0.000  0.000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "world = World(3, 3)\n",
    "world.add_terminal(0, 0, \"+\")\n",
    "Q, policy = mc_epsillon_soft(world, 10000)\n",
    "\n",
    "display(pd.DataFrame(world.grid.T))\n",
    "display(pd.DataFrame(policy).T)\n",
    "display(pd.DataFrame(Q).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d1df40",
   "metadata": {},
   "source": [
    "### Optional exercise\n",
    "\n",
    "Try to implement exploring starts (see page 99 of [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html) for the algorithm). It should be straightforward and only require minimal changes to the code for the exercise above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2fd467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_exploring_starts(world, nb_episodes, gamma=0.9):\n",
    "    policy = {}\n",
    "    Q = {}\n",
    "    returns = {}\n",
    "\n",
    "    # Initialize Q, policy, and returns\n",
    "    for i in range(world.width):\n",
    "        for j in range(world.height):\n",
    "            Q[(i, j)] = {action: 0.0 for action in ACTIONS}\n",
    "            policy[(i, j)] = {action: 1 / len(ACTIONS) for action in ACTIONS}\n",
    "            returns[(i, j)] = {action: [] for action in ACTIONS}\n",
    "\n",
    "    for _ in range(nb_episodes):\n",
    "        # Exploring start - random initial state and action\n",
    "        start_state = (\n",
    "            random.randint(0, world.width - 1),\n",
    "            random.randint(0, world.height - 1),\n",
    "        )\n",
    "        # Do not start in a terminal state or an obstacle\n",
    "        while world.is_terminal(*start_state) or world.is_obstacle(*start_state):\n",
    "            start_state = (\n",
    "                random.randint(0, world.width - 1),\n",
    "                random.randint(0, world.height - 1),\n",
    "            )\n",
    "        episode = generate_episode(world, lambda x, y: policy[(x, y)], start_state)\n",
    "\n",
    "        G = 0\n",
    "        # Iterate forward through the episode\n",
    "        for t, (state, action, reward) in enumerate(reversed(episode)):\n",
    "            idx = len(episode) - t - 1\n",
    "            G = gamma * G + reward\n",
    "            # Check if the (state, action) pair has been seen before in the episode\n",
    "            if (state, action) not in [(s, a) for s, a, _ in episode[:idx]]:\n",
    "                returns[state][action].append(G)\n",
    "                Q[state][action] = np.mean(returns[state][action])\n",
    "\n",
    "                # Update policy to be greedy\n",
    "                best_action = max(Q[state], key=Q[state].get)\n",
    "                for a in ACTIONS:\n",
    "                    policy[state][a] = 1 if a == best_action else 0\n",
    "\n",
    "    return Q, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabfc3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "world = World(5, 5)\n",
    "world.add_terminal(0, 0, \"+\")\n",
    "world.add_terminal(4, 3, \"+\")\n",
    "Q, policy = mc_exploring_starts(world, 10000, gamma)\n",
    "\n",
    "display(pd.DataFrame(world.grid.T))\n",
    "display(pd.DataFrame(policy).T)\n",
    "display(pd.DataFrame(Q).T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
