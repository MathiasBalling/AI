{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47537456",
   "metadata": {},
   "source": [
    "# Temporal difference prediction and control\n",
    "\n",
    "In this notebook, you will implement temporal difference approaches to prediction and control described in [Sutton and Barto's book, Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html). We will use the grid ```World``` class from the previous lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5af8cd",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6515a882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys  # We use sys to get the max value of a float\n",
    "import pandas as pd  # We only use pandas for displaying tables nicely\n",
    "from IPython.display import display\n",
    "\n",
    "pd.options.display.float_format = \"{:,.3f}\".format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8335ed5b",
   "metadata": {},
   "source": [
    "### ```World``` class and globals\n",
    "\n",
    "The ```World``` is a grid represented as a two-dimensional array of characters where each character can represent free space, an obstacle, or a terminal. Each non-obstacle cell is associated with a reward that an agent gets for moving to that cell (can be 0). The size of the world is _width_ $\\times$ _height_ characters.\n",
    "\n",
    "A _state_ is a tuple $(x,y)$.\n",
    "\n",
    "An empty world is created in the ```__init__``` method. Obstacles, rewards and terminals can then be added with ```add_obstacle``` and ```add_reward```.\n",
    "\n",
    "To calculate the next state of an agent (that is, an agent is in some state $s = (x,y)$ and performs and action, $a$), ```get_next_state()```should be called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "252a646b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals:\n",
    "ACTIONS = (\"up\", \"down\", \"left\", \"right\")\n",
    "\n",
    "# Rewards, terminals and obstacles are characters:\n",
    "REWARDS = {\" \": 0, \".\": 0.1, \"+\": 10, \"-\": -10}\n",
    "TERMINALS = (\"+\", \"-\")  # Note a terminal should also have a reward assigned\n",
    "OBSTACLES = \"#\"\n",
    "\n",
    "# Discount factor\n",
    "gamma = 1\n",
    "\n",
    "# The probability of a random move:\n",
    "rand_move_probability = 0\n",
    "\n",
    "\n",
    "class World:\n",
    "    def __init__(self, width, height):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        # Create an empty world where the agent can move to all cells\n",
    "        self.grid = np.full((width, height), \" \", dtype=\"U1\")\n",
    "        # Create an empty windy grid\n",
    "        self.windy_grid = {}\n",
    "        self.initialize_windy_grid()\n",
    "\n",
    "    def initialize_windy_grid(self):\n",
    "        for x in range(self.width):\n",
    "            for y in range(self.height):\n",
    "                self.windy_grid[(x, y)] = (\" \", 0)\n",
    "\n",
    "    def add_obstacle(self, start_x, start_y, end_x=None, end_y=None):\n",
    "        \"\"\"\n",
    "        Create an obstacle in either a single cell or rectangle.\n",
    "        \"\"\"\n",
    "        if end_x == None:\n",
    "            end_x = start_x\n",
    "        if end_y == None:\n",
    "            end_y = start_y\n",
    "\n",
    "        self.grid[start_x : end_x + 1, start_y : end_y + 1] = OBSTACLES[0]\n",
    "\n",
    "    def add_wind(self, x, y, wind_dir, wind_strength):\n",
    "        \"\"\"\n",
    "        Add wind to a cell.\n",
    "        x, y: coordinates of the cell\n",
    "        wind_dir: one of the four cardinal directions\n",
    "        wind_strength: the strength of the wind\n",
    "        \"\"\"\n",
    "        self.windy_grid[(x, y)] = (wind_dir, wind_strength)\n",
    "\n",
    "    def add_reward(self, x, y, reward):\n",
    "        assert reward in REWARDS, f\"{reward} not in {REWARDS}\"\n",
    "        self.grid[x, y] = reward\n",
    "\n",
    "    def add_terminal(self, x, y, terminal):\n",
    "        assert terminal in TERMINALS, f\"{terminal} not in {TERMINALS}\"\n",
    "        self.grid[x, y] = terminal\n",
    "\n",
    "    def is_obstacle(self, x, y):\n",
    "        if x < 0 or x >= self.width or y < 0 or y >= self.height:\n",
    "            return True\n",
    "        else:\n",
    "            return self.grid[x, y] in OBSTACLES\n",
    "\n",
    "    def is_terminal(self, x, y):\n",
    "        return self.grid[x, y] in TERMINALS\n",
    "\n",
    "    def get_reward(self, x, y):\n",
    "        \"\"\"\n",
    "        Return the reward associated with a given location\n",
    "        \"\"\"\n",
    "        return REWARDS[self.grid[x, y]]\n",
    "\n",
    "    def get_next_state(self, current_state, action):\n",
    "        \"\"\"\n",
    "        Get the next state given a current state and an action. The outcome can be\n",
    "        stochastic  where rand_move_probability determines the probability of\n",
    "        ignoring the action and performing a random move.\n",
    "        \"\"\"\n",
    "        assert action in ACTIONS, f\"Unknown acion {action} must be one of {ACTIONS}\"\n",
    "\n",
    "        x, y = current_state\n",
    "\n",
    "        # If our current state is a terminal, there is no next state\n",
    "        if self.grid[x, y] in TERMINALS:\n",
    "            return None\n",
    "\n",
    "        # Check of a random action should be performed:\n",
    "        if np.random.rand() < rand_move_probability:\n",
    "            action = np.random.choice(ACTIONS)\n",
    "\n",
    "        if action == \"up\":\n",
    "            y -= 1\n",
    "        elif action == \"down\":\n",
    "            y += 1\n",
    "        elif action == \"left\":\n",
    "            x -= 1\n",
    "        elif action == \"right\":\n",
    "            x += 1\n",
    "        elif action == \"up-left\":\n",
    "            x -= 1\n",
    "            y -= 1\n",
    "        elif action == \"up-right\":\n",
    "            x += 1\n",
    "            y -= 1\n",
    "        elif action == \"down-left\":\n",
    "            x -= 1\n",
    "            y += 1\n",
    "        elif action == \"down-right\":\n",
    "            x += 1\n",
    "            y += 1\n",
    "\n",
    "        # If the next state is an obstacle, stay in the current state\n",
    "        return (x, y) if not self.is_obstacle(x, y) else current_state\n",
    "\n",
    "    def get_next_state_windy(self, current_state, action):\n",
    "        \"\"\"\n",
    "        Get the next state given a current state and an action. The outcome can be\n",
    "        stochastic  where rand_move_probability determines the probability of\n",
    "        ignoring the action and performing a random move.\n",
    "        \"\"\"\n",
    "        assert action in ACTIONS, f\"Unknown acion {action} must be one of {ACTIONS}\"\n",
    "\n",
    "        x, y = current_state\n",
    "\n",
    "        # If our current state is a terminal, there is no next state\n",
    "        if self.grid[x, y] in TERMINALS:\n",
    "            return None\n",
    "\n",
    "        # Check of a random action should be performed:\n",
    "        if np.random.rand() < rand_move_probability:\n",
    "            action = np.random.choice(ACTIONS)\n",
    "\n",
    "        if self.windy_grid[x, y][0] == \"up\":\n",
    "            y -= self.windy_grid[x, y][1]\n",
    "        elif self.windy_grid[x, y][0] == \"down\":\n",
    "            y += self.windy_grid[x, y][1]\n",
    "        elif self.windy_grid[x, y][0] == \"left\":\n",
    "            x -= self.windy_grid[x, y][1]\n",
    "        elif self.windy_grid[x, y][0] == \"right\":\n",
    "            x += self.windy_grid[x, y][1]\n",
    "\n",
    "        if action == \"up\":\n",
    "            y -= 1\n",
    "        elif action == \"down\":\n",
    "            y += 1\n",
    "        elif action == \"left\":\n",
    "            x -= 1\n",
    "        elif action == \"right\":\n",
    "            x += 1\n",
    "        elif action == \"up-left\":\n",
    "            x -= 1\n",
    "            y -= 1\n",
    "        elif action == \"up-right\":\n",
    "            x += 1\n",
    "            y -= 1\n",
    "        elif action == \"down-left\":\n",
    "            x -= 1\n",
    "            y += 1\n",
    "        elif action == \"down-right\":\n",
    "            x += 1\n",
    "            y += 1\n",
    "\n",
    "        # Limit values to be within the grid\n",
    "        x = min(max(x, 0), self.width - 1)\n",
    "        y = min(max(y, 0), self.height - 1)\n",
    "\n",
    "        # If the next state is an obstacle, stay in the current state\n",
    "        return (x, y) if not self.is_obstacle(x, y) else current_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c75ecbc",
   "metadata": {},
   "source": [
    "## A simple world and a simple policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fe89cddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ']\n",
      " [' ' ' ']\n",
      " [' ' '+']]\n"
     ]
    }
   ],
   "source": [
    "ACTIONS = (\"up\", \"down\", \"left\", \"right\")\n",
    "world = World(2, 3)\n",
    "\n",
    "# Since we only focus on episodic tasks, we must have a terminal state that the\n",
    "# agent eventually reaches\n",
    "world.add_terminal(1, 2, \"+\")\n",
    "\n",
    "\n",
    "def equiprobable_random_policy(x, y):\n",
    "    return {k: 1 / len(ACTIONS) for k in ACTIONS}\n",
    "\n",
    "\n",
    "print(world.grid.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1536b9c1",
   "metadata": {},
   "source": [
    "## Exercise: TD prediction\n",
    "\n",
    "You should implement TD prediction for estimating $V≈v_\\pi$. See page 120 of [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html).\n",
    "\n",
    "\n",
    "To implement TD prediction, the agent has to interact with the world for a certain number of episodes. However, unlike in the Monte Carlo case, we do not rely on complete sample runs, but instead update estimates (for prediction and control) and the policy (for control only) each time step in an episode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219389c0",
   "metadata": {},
   "source": [
    "Below, you can see the code for running an episode, with a TODO where you have to add your code for prediction. Also, play with the parameters ```alpha``` and ```EPISODES```, you will typically need a lot more than 10 episodes for an agent to learn anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "898c0aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/10:\n",
      "Current state (S) = (0, 0), next_state S' = (1, 0), reward = 0\n",
      "Current state (S) = (1, 0), next_state S' = (1, 0), reward = 0\n",
      "Current state (S) = (1, 0), next_state S' = (1, 0), reward = 0\n",
      "Current state (S) = (1, 0), next_state S' = (1, 0), reward = 0\n",
      "Current state (S) = (1, 0), next_state S' = (1, 0), reward = 0\n",
      "Current state (S) = (1, 0), next_state S' = (1, 0), reward = 0\n",
      "Current state (S) = (1, 0), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (1, 0), reward = 0\n",
      "Current state (S) = (1, 0), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (1, 1), reward = 0\n",
      "Current state (S) = (1, 1), next_state S' = (1, 1), reward = 0\n",
      "Current state (S) = (1, 1), next_state S' = (1, 0), reward = 0\n",
      "Current state (S) = (1, 0), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 2), reward = 0\n",
      "Current state (S) = (0, 2), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 2), reward = 0\n",
      "Current state (S) = (0, 2), next_state S' = (0, 2), reward = 0\n",
      "Current state (S) = (0, 2), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (1, 1), reward = 0\n",
      "Current state (S) = (1, 1), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (1, 1), reward = 0\n",
      "Current state (S) = (1, 1), next_state S' = (1, 2), reward = 10\n",
      "Episode 2/10:\n",
      "Current state (S) = (0, 0), next_state S' = (1, 0), reward = 0\n",
      "Current state (S) = (1, 0), next_state S' = (1, 0), reward = 0\n",
      "Current state (S) = (1, 0), next_state S' = (1, 0), reward = 0\n",
      "Current state (S) = (1, 0), next_state S' = (1, 0), reward = 0\n",
      "Current state (S) = (1, 0), next_state S' = (1, 0), reward = 0\n",
      "Current state (S) = (1, 0), next_state S' = (1, 0), reward = 0\n",
      "Current state (S) = (1, 0), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (1, 1), reward = 0\n",
      "Current state (S) = (1, 1), next_state S' = (1, 2), reward = 10\n",
      "Episode 3/10:\n",
      "Current state (S) = (0, 0), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 2), reward = 0\n",
      "Current state (S) = (0, 2), next_state S' = (0, 2), reward = 0\n",
      "Current state (S) = (0, 2), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (1, 1), reward = 0\n",
      "Current state (S) = (1, 1), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (1, 0), reward = 0\n",
      "Current state (S) = (1, 0), next_state S' = (1, 1), reward = 0\n",
      "Current state (S) = (1, 1), next_state S' = (1, 0), reward = 0\n",
      "Current state (S) = (1, 0), next_state S' = (1, 1), reward = 0\n",
      "Current state (S) = (1, 1), next_state S' = (1, 1), reward = 0\n",
      "Current state (S) = (1, 1), next_state S' = (1, 1), reward = 0\n",
      "Current state (S) = (1, 1), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 2), reward = 0\n",
      "Current state (S) = (0, 2), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (1, 0), reward = 0\n",
      "Current state (S) = (1, 0), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 2), reward = 0\n",
      "Current state (S) = (0, 2), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (1, 0), reward = 0\n",
      "Current state (S) = (1, 0), next_state S' = (1, 0), reward = 0\n",
      "Current state (S) = (1, 0), next_state S' = (1, 0), reward = 0\n",
      "Current state (S) = (1, 0), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (1, 0), reward = 0\n",
      "Current state (S) = (1, 0), next_state S' = (1, 0), reward = 0\n",
      "Current state (S) = (1, 0), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (1, 0), reward = 0\n",
      "Current state (S) = (1, 0), next_state S' = (1, 1), reward = 0\n",
      "Current state (S) = (1, 1), next_state S' = (1, 1), reward = 0\n",
      "Current state (S) = (1, 1), next_state S' = (1, 2), reward = 10\n",
      "Episode 4/10:\n",
      "Current state (S) = (0, 0), next_state S' = (1, 0), reward = 0\n",
      "Current state (S) = (1, 0), next_state S' = (1, 1), reward = 0\n",
      "Current state (S) = (1, 1), next_state S' = (1, 1), reward = 0\n",
      "Current state (S) = (1, 1), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 2), reward = 0\n",
      "Current state (S) = (0, 2), next_state S' = (0, 2), reward = 0\n",
      "Current state (S) = (0, 2), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 2), reward = 0\n",
      "Current state (S) = (0, 2), next_state S' = (1, 2), reward = 10\n",
      "Episode 5/10:\n",
      "Current state (S) = (0, 0), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 2), reward = 0\n",
      "Current state (S) = (0, 2), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (1, 1), reward = 0\n",
      "Current state (S) = (1, 1), next_state S' = (1, 1), reward = 0\n",
      "Current state (S) = (1, 1), next_state S' = (1, 0), reward = 0\n",
      "Current state (S) = (1, 0), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (1, 1), reward = 0\n",
      "Current state (S) = (1, 1), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 2), reward = 0\n",
      "Current state (S) = (0, 2), next_state S' = (1, 2), reward = 10\n",
      "Episode 6/10:\n",
      "Current state (S) = (0, 0), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (1, 1), reward = 0\n",
      "Current state (S) = (1, 1), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 2), reward = 0\n",
      "Current state (S) = (0, 2), next_state S' = (0, 2), reward = 0\n",
      "Current state (S) = (0, 2), next_state S' = (0, 2), reward = 0\n",
      "Current state (S) = (0, 2), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 2), reward = 0\n",
      "Current state (S) = (0, 2), next_state S' = (1, 2), reward = 10\n",
      "Episode 7/10:\n",
      "Current state (S) = (0, 0), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (1, 0), reward = 0\n",
      "Current state (S) = (1, 0), next_state S' = (1, 1), reward = 0\n",
      "Current state (S) = (1, 1), next_state S' = (1, 1), reward = 0\n",
      "Current state (S) = (1, 1), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 2), reward = 0\n",
      "Current state (S) = (0, 2), next_state S' = (1, 2), reward = 10\n",
      "Episode 8/10:\n",
      "Current state (S) = (0, 0), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (1, 0), reward = 0\n",
      "Current state (S) = (1, 0), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 2), reward = 0\n",
      "Current state (S) = (0, 2), next_state S' = (0, 2), reward = 0\n",
      "Current state (S) = (0, 2), next_state S' = (1, 2), reward = 10\n",
      "Episode 9/10:\n",
      "Current state (S) = (0, 0), next_state S' = (1, 0), reward = 0\n",
      "Current state (S) = (1, 0), next_state S' = (1, 0), reward = 0\n",
      "Current state (S) = (1, 0), next_state S' = (1, 0), reward = 0\n",
      "Current state (S) = (1, 0), next_state S' = (1, 1), reward = 0\n",
      "Current state (S) = (1, 1), next_state S' = (1, 1), reward = 0\n",
      "Current state (S) = (1, 1), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (1, 1), reward = 0\n",
      "Current state (S) = (1, 1), next_state S' = (1, 0), reward = 0\n",
      "Current state (S) = (1, 0), next_state S' = (1, 0), reward = 0\n",
      "Current state (S) = (1, 0), next_state S' = (1, 0), reward = 0\n",
      "Current state (S) = (1, 0), next_state S' = (1, 0), reward = 0\n",
      "Current state (S) = (1, 0), next_state S' = (1, 0), reward = 0\n",
      "Current state (S) = (1, 0), next_state S' = (1, 0), reward = 0\n",
      "Current state (S) = (1, 0), next_state S' = (1, 0), reward = 0\n",
      "Current state (S) = (1, 0), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (1, 0), reward = 0\n",
      "Current state (S) = (1, 0), next_state S' = (1, 0), reward = 0\n",
      "Current state (S) = (1, 0), next_state S' = (1, 1), reward = 0\n",
      "Current state (S) = (1, 1), next_state S' = (1, 1), reward = 0\n",
      "Current state (S) = (1, 1), next_state S' = (1, 1), reward = 0\n",
      "Current state (S) = (1, 1), next_state S' = (1, 2), reward = 10\n",
      "Episode 10/10:\n",
      "Current state (S) = (0, 0), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (0, 0), reward = 0\n",
      "Current state (S) = (0, 0), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (0, 2), reward = 0\n",
      "Current state (S) = (0, 2), next_state S' = (0, 2), reward = 0\n",
      "Current state (S) = (0, 2), next_state S' = (0, 1), reward = 0\n",
      "Current state (S) = (0, 1), next_state S' = (1, 1), reward = 0\n",
      "Current state (S) = (1, 1), next_state S' = (1, 2), reward = 10\n"
     ]
    }
   ],
   "source": [
    "# Global variable to keep track of current estimates\n",
    "V = {}\n",
    "\n",
    "# Our step size / learing rate\n",
    "alpha = 0.05\n",
    "\n",
    "# Discount factor\n",
    "gamma = 0.9\n",
    "\n",
    "# Episodes to run\n",
    "EPISODES = 10\n",
    "\n",
    "\n",
    "def TD_prediction_run_episode(world, policy, start_state):\n",
    "    current_state = start_state\n",
    "    while not world.is_terminal(*current_state):\n",
    "        # Get the possible actions and their probabilities that our policy says\n",
    "        # that the agent should perform in the current state:\n",
    "        possible_actions = policy(*current_state)\n",
    "\n",
    "        # Pick a weighted random action:\n",
    "        action = random.choices(\n",
    "            population=list(possible_actions.keys()),\n",
    "            weights=possible_actions.values(),\n",
    "            k=1,\n",
    "        )\n",
    "\n",
    "        # Get the next state from the world\n",
    "        next_state = world.get_next_state(current_state, action[0])\n",
    "\n",
    "        # Get the reward for performing the action\n",
    "        reward = world.get_reward(*next_state)\n",
    "\n",
    "        if current_state not in V:\n",
    "            V[current_state] = 0\n",
    "        if next_state not in V:\n",
    "            V[current_state] = 0\n",
    "\n",
    "        print(\n",
    "            f\"Current state (S) = {current_state}, next_state S' = {next_state}, reward = {reward}\"\n",
    "        )\n",
    "\n",
    "        # Move the agent to the new state\n",
    "        current_state = next_state\n",
    "\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    print(f\"Episode {episode + 1 }/{EPISODES}:\")\n",
    "    TD_prediction_run_episode(world, equiprobable_random_policy, (0, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e068ad4",
   "metadata": {},
   "source": [
    "## Exercise: SARSA\n",
    "\n",
    "Implement and test SARSA with an $\\epsilon$-greedy policy. See page 130 of [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html) on different worlds. Make sure that it is easy to show a learnt policy (most probable action in each state).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "75522e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(Q, state, epsilon):\n",
    "    \"\"\"\n",
    "    Selects an action using an epsilon-greedy policy.\n",
    "\n",
    "    Args:\n",
    "        Q (dict): The Q-table containing state-action values.\n",
    "        state (tuple): The current state.\n",
    "        epsilon (float): The probability of selecting a random action (exploration).\n",
    "\n",
    "    Returns:\n",
    "        action (str): The action selected.\n",
    "    \"\"\"\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(ACTIONS)\n",
    "    else:\n",
    "        if state not in Q:\n",
    "            Q[state] = {action: 0.0 for action in ACTIONS}\n",
    "        return max(Q[state], key=Q[state].get)\n",
    "\n",
    "\n",
    "def SARSA(world: World, n_episodes, gamma=0.9, alpha=0.1, epsilon=0.1, use_wind=False):\n",
    "    \"\"\"\n",
    "    Implements the SARSA algorithm for on-policy TD control.\n",
    "\n",
    "    Args:\n",
    "        world (World): The environment in which the agent operates.\n",
    "        n_episodes (int): The number of episodes to run the algorithm.\n",
    "        gamma (float): The discount factor.\n",
    "        alpha (float): The learning rate.\n",
    "        epsilon (float): The probability of selecting a random action (exploration).\n",
    "\n",
    "    Returns:\n",
    "        Q (dict): The Q-table containing state-action values.\n",
    "    \"\"\"\n",
    "    Q = {}\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        # Initialize the starting state\n",
    "        start_state = (\n",
    "            random.randint(0, world.width - 1),\n",
    "            random.randint(0, world.height - 1),\n",
    "        )\n",
    "        # Make sure the starting state is not a terminal or obstacle\n",
    "        while world.is_terminal(*start_state) or world.is_obstacle(*start_state):\n",
    "            start_state = (\n",
    "                random.randint(0, world.width - 1),\n",
    "                random.randint(0, world.height - 1),\n",
    "            )\n",
    "\n",
    "        # Initialize the starting action\n",
    "        current_action = greedy_policy(Q, start_state, epsilon)\n",
    "\n",
    "        current_state = start_state\n",
    "        while not world.is_terminal(*current_state):\n",
    "            # Get the next state, action, and reward\n",
    "            if use_wind:\n",
    "                next_state = world.get_next_state_windy(current_state, current_action)\n",
    "            else:\n",
    "                next_state = world.get_next_state(current_state, current_action)\n",
    "\n",
    "            next_action = greedy_policy(Q, next_state, epsilon)\n",
    "            reward = world.get_reward(*next_state)\n",
    "\n",
    "            # Helper to initialize Q-values for new states and actions\n",
    "            if current_state not in Q:\n",
    "                Q[current_state] = {action: 0.0 for action in ACTIONS}\n",
    "            if next_state not in Q:\n",
    "                Q[next_state] = {action: 0.0 for action in ACTIONS}\n",
    "\n",
    "            # Update the Q-value for the current state-action pair\n",
    "            Q[current_state][current_action] += alpha * (\n",
    "                reward\n",
    "                + gamma * Q[next_state][next_action]\n",
    "                - Q[current_state][current_action]\n",
    "            )\n",
    "\n",
    "            # Move to the next state and action\n",
    "            current_action = next_action\n",
    "            current_state = next_state\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fef391a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>+</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2\n",
       "0         \n",
       "1         \n",
       "2     +   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>down</td>\n",
       "      <td>down</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>down</td>\n",
       "      <td>down</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>right</td>\n",
       "      <td>termnal</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0        1     2\n",
       "0   down     down  down\n",
       "1   down     down  down\n",
       "2  right  termnal  left"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ACTIONS = (\"up\", \"down\", \"left\", \"right\")\n",
    "world = World(3, 3)\n",
    "world.add_terminal(1, 2, \"+\")\n",
    "display(pd.DataFrame(world.grid.T))\n",
    "\n",
    "Q = SARSA(world, 50000)\n",
    "final_policy = np.full((world.width, world.height), \"          \")\n",
    "for i in range(world.width):\n",
    "    for j in range(world.height):\n",
    "        if world.is_terminal(i, j):\n",
    "            final_policy[(i, j)] = \"termnal\"\n",
    "        elif world.is_obstacle(i, j):\n",
    "            final_policy[(i, j)] = \"#\"\n",
    "        else:\n",
    "            final_policy[(i, j)] = max(Q[(i, j)], key=Q[(i, j)].get)\n",
    "display(pd.DataFrame(final_policy.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c9b627",
   "metadata": {},
   "source": [
    "## Exercise: Windy Gridworld\n",
    "\n",
    "Implement the Windy Gridworld (Example 6.5 on page 130 in the book) and test your SARSA implementation on the Windy Gridworld, first with the four actions (```up, down, left, right```) that move the agent in the cardinal directions, and then with King's moves as described in Exercise 6.9. How long does it take to learn a good policy for different values of $\\alpha$ and $\\epsilon$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13d52d2",
   "metadata": {},
   "source": [
    "### Without King's Moves (Windy Gridworld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cf488dfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4\n",
       "0               \n",
       "1               \n",
       "2               \n",
       "3               \n",
       "4              +"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{(0, 0): ('', 0),\n",
       " (0, 1): ('', 0),\n",
       " (0, 2): ('', 0),\n",
       " (0, 3): ('', 0),\n",
       " (0, 4): ('', 0),\n",
       " (1, 0): ('', 0),\n",
       " (1, 1): ('', 0),\n",
       " (1, 2): ('', 0),\n",
       " (1, 3): ('', 0),\n",
       " (1, 4): ('', 0),\n",
       " (2, 0): ('up', 1),\n",
       " (2, 1): ('up', 1),\n",
       " (2, 2): ('up', 1),\n",
       " (2, 3): ('up', 1),\n",
       " (2, 4): ('up', 1),\n",
       " (3, 0): ('', 0),\n",
       " (3, 1): ('', 0),\n",
       " (3, 2): ('', 0),\n",
       " (3, 3): ('', 0),\n",
       " (3, 4): ('', 0),\n",
       " (4, 0): ('', 0),\n",
       " (4, 1): ('', 0),\n",
       " (4, 2): ('', 0),\n",
       " (4, 3): ('', 0),\n",
       " (4, 4): ('', 0)}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>right</td>\n",
       "      <td>right</td>\n",
       "      <td>right</td>\n",
       "      <td>right</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>down</td>\n",
       "      <td>right</td>\n",
       "      <td>right</td>\n",
       "      <td>right</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>right</td>\n",
       "      <td>down</td>\n",
       "      <td>right</td>\n",
       "      <td>right</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>right</td>\n",
       "      <td>right</td>\n",
       "      <td>right</td>\n",
       "      <td>right</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>right</td>\n",
       "      <td>right</td>\n",
       "      <td>right</td>\n",
       "      <td>right</td>\n",
       "      <td>termnal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3        4\n",
       "0  right  right  right  right     down\n",
       "1   down  right  right  right     down\n",
       "2  right   down  right  right     down\n",
       "3  right  right  right  right     down\n",
       "4  right  right  right  right  termnal"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ACTIONS = (\"up\", \"down\", \"left\", \"right\")\n",
    "world = World(width=5, height=5)\n",
    "\n",
    "# Add wind to the grid in x=2 with strength 1\n",
    "for i in range(world.height):\n",
    "    world.add_wind(2, i, \"up\", 1)\n",
    "\n",
    "world.add_terminal(4, 4, \"+\")\n",
    "\n",
    "display(pd.DataFrame(world.grid.T))\n",
    "\n",
    "Q = SARSA(world, 30000, use_wind=True)\n",
    "final_policy = np.full((world.width, world.height), \"          \")\n",
    "for i in range(world.width):\n",
    "    for j in range(world.height):\n",
    "        if world.is_terminal(i, j):\n",
    "            final_policy[(i, j)] = \"termnal\"\n",
    "        elif world.is_obstacle(i, j):\n",
    "            final_policy[(i, j)] = \"#\"\n",
    "        else:\n",
    "            final_policy[(i, j)] = max(Q[(i, j)], key=Q[(i, j)].get)\n",
    "display(pd.DataFrame(final_policy.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19750639",
   "metadata": {},
   "source": [
    "### With King's Moves (Windy Gridworld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "690600b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>up-right</td>\n",
       "      <td>down-right</td>\n",
       "      <td>right</td>\n",
       "      <td>down</td>\n",
       "      <td>down-left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>up-right</td>\n",
       "      <td>down-right</td>\n",
       "      <td>down-right</td>\n",
       "      <td>down</td>\n",
       "      <td>down-right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>down-left</td>\n",
       "      <td>up-right</td>\n",
       "      <td>down-right</td>\n",
       "      <td>down-right</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>down-right</td>\n",
       "      <td>up-right</td>\n",
       "      <td>up</td>\n",
       "      <td>down-right</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>up</td>\n",
       "      <td>right</td>\n",
       "      <td>down-right</td>\n",
       "      <td>right</td>\n",
       "      <td>termnal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0           1           2           3           4\n",
       "0    up-right  down-right       right        down   down-left\n",
       "1    up-right  down-right  down-right        down  down-right\n",
       "2   down-left    up-right  down-right  down-right        down\n",
       "3  down-right    up-right          up  down-right        down\n",
       "4          up       right  down-right       right     termnal"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ACTIONS = (\n",
    "    \"up\",\n",
    "    \"down\",\n",
    "    \"left\",\n",
    "    \"right\",\n",
    "    \"up-left\",\n",
    "    \"up-right\",\n",
    "    \"down-left\",\n",
    "    \"down-right\",\n",
    ")\n",
    "Q = SARSA(world, 30000, use_wind=True)\n",
    "final_policy = np.full((world.width, world.height), \"          \")\n",
    "for i in range(world.width):\n",
    "    for j in range(world.height):\n",
    "        if world.is_terminal(i, j):\n",
    "            final_policy[(i, j)] = \"termnal\"\n",
    "        elif world.is_obstacle(i, j):\n",
    "            final_policy[(i, j)] = \"#\"\n",
    "        else:\n",
    "            final_policy[(i, j)] = max(Q[(i, j)], key=Q[(i, j)].get)\n",
    "display(pd.DataFrame(final_policy.T))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
