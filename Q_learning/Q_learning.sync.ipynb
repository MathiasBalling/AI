{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e002ba54",
   "metadata": {},
   "source": [
    "# Q-learning\n",
    "\n",
    "In this notebook, you will implement Q-learning as described in [Sutton and Barto's book, Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html). We will use the grid ```World``` class from the previous lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ff01a4",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d58d34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/balling/projects/AI/env/lib/python3.9/site-packages (2.0.2)\n",
      "Requirement already satisfied: pandas in /Users/balling/projects/AI/env/lib/python3.9/site-packages (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/balling/projects/AI/env/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/balling/projects/AI/env/lib/python3.9/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/balling/projects/AI/env/lib/python3.9/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/balling/projects/AI/env/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install numpy pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ab4ce0",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e064d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys  # We use sys to get the max value of a float\n",
    "from IPython.display import display\n",
    "import pandas as pd  # We only use pandas for displaying tables nicely\n",
    "\n",
    "pd.options.display.float_format = \"{:,.3f}\".format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0097a47",
   "metadata": {},
   "source": [
    "### ```World``` class and globals\n",
    "\n",
    "The ```World``` is a grid represented as a two-dimensional array of characters where each character can represent free space, an obstacle, or a terminal. Each non-obstacle cell is associated with a reward that an agent gets for moving to that cell (can be 0). The size of the world is _width_ $\\times$ _height_ characters.\n",
    "\n",
    "A _state_ is a tuple $(x,y)$.\n",
    "\n",
    "An empty world is created in the ```__init__``` method. Obstacles, rewards and terminals can then be added with ```add_obstacle``` and ```add_reward```.\n",
    "\n",
    "To calculate the next state of an agent (that is, an agent is in some state $s = (x,y)$ and performs and action, $a$), ```get_next_state()```should be called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24bdaff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals:\n",
    "ACTIONS = (\"up\", \"down\", \"left\", \"right\")\n",
    "\n",
    "# Rewards, terminals and obstacles are characters:\n",
    "REWARDS = {\" \": 0, \".\": 0.1, \"+\": 10, \"-\": -10}\n",
    "TERMINALS = (\"+\", \"-\")  # Note a terminal should also have a reward assigned\n",
    "OBSTACLES = \"#\"\n",
    "\n",
    "# Discount factor\n",
    "gamma = 1\n",
    "\n",
    "# The probability of a random move:\n",
    "rand_move_probability = 0\n",
    "\n",
    "\n",
    "class World:\n",
    "    def __init__(self, width, height):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        # Create an empty world where the agent can move to all cells\n",
    "        self.grid = np.full((width, height), \" \", dtype=\"U1\")\n",
    "\n",
    "    def add_obstacle(self, start_x, start_y, end_x=None, end_y=None):\n",
    "        \"\"\"\n",
    "        Create an obstacle in either a single cell or rectangle.\n",
    "        \"\"\"\n",
    "        if end_x == None:\n",
    "            end_x = start_x\n",
    "        if end_y == None:\n",
    "            end_y = start_y\n",
    "\n",
    "        self.grid[start_x : end_x + 1, start_y : end_y + 1] = OBSTACLES[0]\n",
    "\n",
    "    def add_reward(self, x, y, reward):\n",
    "        assert reward in REWARDS, f\"{reward} not in {REWARDS}\"\n",
    "        self.grid[x, y] = reward\n",
    "\n",
    "    def add_terminal(self, x, y, terminal):\n",
    "        assert terminal in TERMINALS, f\"{terminal} not in {TERMINALS}\"\n",
    "        self.grid[x, y] = terminal\n",
    "\n",
    "    def is_obstacle(self, x, y):\n",
    "        if x < 0 or x >= self.width or y < 0 or y >= self.height:\n",
    "            return True\n",
    "        else:\n",
    "            return self.grid[x, y] in OBSTACLES\n",
    "\n",
    "    def is_terminal(self, x, y):\n",
    "        return self.grid[x, y] in TERMINALS\n",
    "\n",
    "    def get_reward(self, x, y):\n",
    "        \"\"\"\n",
    "        Return the reward associated with a given location\n",
    "        \"\"\"\n",
    "        return REWARDS[self.grid[x, y]]\n",
    "\n",
    "    def get_next_state(self, current_state, action):\n",
    "        \"\"\"\n",
    "        Get the next state given a current state and an action. The outcome can be\n",
    "        stochastic  where rand_move_probability determines the probability of\n",
    "        ignoring the action and performing a random move.\n",
    "        \"\"\"\n",
    "        assert action in ACTIONS, f\"Unknown acion {action} must be one of {ACTIONS}\"\n",
    "\n",
    "        x, y = current_state\n",
    "\n",
    "        # If our current state is a terminal, there is no next state\n",
    "        if self.grid[x, y] in TERMINALS:\n",
    "            return None\n",
    "\n",
    "        # Check of a random action should be performed:\n",
    "        if np.random.rand() < rand_move_probability:\n",
    "            action = np.random.choice(ACTIONS)\n",
    "\n",
    "        if action == \"up\":\n",
    "            y -= 1\n",
    "        elif action == \"down\":\n",
    "            y += 1\n",
    "        elif action == \"left\":\n",
    "            x -= 1\n",
    "        elif action == \"right\":\n",
    "            x += 1\n",
    "\n",
    "        # If the next state is an obstacle, stay in the current state\n",
    "        return (x, y) if not self.is_obstacle(x, y) else current_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60c7ddb",
   "metadata": {},
   "source": [
    "## A simple world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7da198ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' '+']]\n"
     ]
    }
   ],
   "source": [
    "world = World(4, 6)\n",
    "\n",
    "# Since we only focus on episodic tasks, we must have a terminal state that the\n",
    "# agent eventually reaches\n",
    "world.add_terminal(3, 5, \"+\")\n",
    "\n",
    "print(world.grid.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822d3a56",
   "metadata": {},
   "source": [
    "## Exercise: Q-learning\n",
    "\n",
    "Implement and test Q-learning. You should be able to base much of your code on your implementation of SARSA. Since Q-learning is an off-policy method, we can use whatever behavior policy we want during training, but the choice of behavioral policy still manners so it is a good idea to balance exploration and exploitation. During testing, we can then use the learnt policy (the target policy).\n",
    "\n",
    "As for the behavior policy, you can use an simple $\\epsilon$-greedy policy, but you can also experiment with alternatives, for instance, optimistic initial values.\n",
    "\n",
    "See page 131 in [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html) for the Q-learning algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9311557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(Q, state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(ACTIONS)\n",
    "    else:\n",
    "        if state not in Q:\n",
    "            Q[state] = {action: 0.0 for action in ACTIONS}\n",
    "        return max(Q[state], key=Q[state].get)\n",
    "\n",
    "\n",
    "def q_learning(world: World, n_episodes, gamma=0.9, alpha=0.1, epsilon=0.1):\n",
    "    Q = {}\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        # Initialize the start state\n",
    "        start_state = (\n",
    "            random.randint(0, world.width - 1),\n",
    "            random.randint(0, world.height - 1),\n",
    "        )\n",
    "        # Make sure the start state is not a terminal or obstacle\n",
    "        while world.is_terminal(*start_state) or world.is_obstacle(*start_state):\n",
    "            start_state = (\n",
    "                random.randint(0, world.width - 1),\n",
    "                random.randint(0, world.height - 1),\n",
    "            )\n",
    "\n",
    "        current_state = start_state\n",
    "        while not world.is_terminal(*current_state):\n",
    "            # Choose the next action based on the epsilon-greedy policy\n",
    "            current_action = greedy_policy(Q, start_state, epsilon)\n",
    "\n",
    "            # Get the next state and reward\n",
    "            next_state = world.get_next_state(current_state, current_action)\n",
    "            reward = world.get_reward(*next_state)\n",
    "\n",
    "            # Helper to create the Q-table entries if they do not exist\n",
    "            if current_state not in Q:\n",
    "                Q[current_state] = {action: 0.0 for action in ACTIONS}\n",
    "            if next_state not in Q:\n",
    "                Q[next_state] = {action: 0.0 for action in ACTIONS}\n",
    "\n",
    "            # Find the best next action in the next state\n",
    "            best_next_action_value = max(Q[next_state].values())\n",
    "\n",
    "            # Update the Q-table\n",
    "            Q[current_state][current_action] += alpha * (\n",
    "                reward\n",
    "                + gamma * best_next_action_value\n",
    "                - Q[current_state][current_action]\n",
    "            )\n",
    "\n",
    "            # Update the state\n",
    "            current_state = next_state\n",
    "\n",
    "    # Return the Q-table after training to be used as a policy\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3a0cdb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>+</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2\n",
       "0         \n",
       "1         \n",
       "2     +   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>right</td>\n",
       "      <td>right</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>right</td>\n",
       "      <td>down</td>\n",
       "      <td>down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>right</td>\n",
       "      <td>termnal</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0        1     2\n",
       "0  right    right  down\n",
       "1  right     down  down\n",
       "2  right  termnal  left"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ACTIONS = (\"up\", \"down\", \"left\", \"right\")\n",
    "world = World(3, 3)\n",
    "world.add_terminal(1, 2, \"+\")\n",
    "display(pd.DataFrame(world.grid.T))\n",
    "\n",
    "Q = q_learning(world, 100, gamma=0.9, alpha=0.1, epsilon=0.1)\n",
    "final_policy = np.full((world.width, world.height), \"          \")\n",
    "for i in range(world.width):\n",
    "    for j in range(world.height):\n",
    "        if world.is_terminal(i, j):\n",
    "            final_policy[(i, j)] = \"termnal\"\n",
    "        elif world.is_obstacle(i, j):\n",
    "            final_policy[(i, j)] = \"#\"\n",
    "        else:\n",
    "            final_policy[(i, j)] = max(Q[(i, j)], key=Q[(i, j)].get)\n",
    "display(pd.DataFrame(final_policy.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e151bbd",
   "metadata": {},
   "source": [
    "## Exercise: Compare Q-learning and SARSA\n",
    "\n",
    "Setup experiments to compare the performance of Q-learning and SARSA. You can use different ```Worlds``` and test different parameter setting, e.g. for $\\alpha$ and $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e28d13a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Implement your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12733ca",
   "metadata": {},
   "source": [
    "## Optional exercise: Maximization Bias and Double Learning\n",
    "\n",
    "Below is an implementation of the task shown in Example 6.7 on page 134 in [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html). There are two states, ```A``` and ```B``` where the agent can perform actions, and a terminal state ```T```. ```A``` and ```B``` have different actions available:\n",
    "\n",
    "* ```A``` has ```left``` (to ```B```) and ```right``` to the terminal state\n",
    "* ```B``` has a larger number of actions all leading to a terminal state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67552b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# States \"A\" and \"B\" have actions while \"T\" is a terminal state.\n",
    "STATES = (\"A\", \"B\", \"T\")\n",
    "\n",
    "\n",
    "class Example67MDP:\n",
    "    def __init__(self, number_of_B_actions):\n",
    "        \"\"\"\n",
    "        Create an example and set the number of outgoing actions for state \"B\"\n",
    "        (in the book, they do not give a specific number, but merely write that\n",
    "        from \"B\" there \"are many possible actions all of which cause immediate\n",
    "        termination with a reward drawn from a normal distribution with mean\n",
    "        -0.1 and variance 1. So, feel free to play with different number of\n",
    "        actions in state B)\n",
    "\n",
    "        \"\"\"\n",
    "        self.number_of_B_actions = number_of_B_actions\n",
    "\n",
    "    def get_actions(self, state):\n",
    "        \"\"\"\n",
    "        Returns the set of actions availabe in a given state (a tuple\n",
    "        with strings).\n",
    "        \"\"\"\n",
    "        assert state in STATES, f\"State must be one of {STATES}, not {state}\"\n",
    "        if state == \"A\":\n",
    "            return (\"left\", \"right\")\n",
    "        if state == \"B\":\n",
    "            return tuple(f\"{i}\" for i in range(self.number_of_B_actions))\n",
    "        if state == \"T\":\n",
    "            return tuple(\"N\")\n",
    "\n",
    "    def get_next_state_and_reward(self, state, action):\n",
    "        \"\"\"\n",
    "        Get the next state and reward given a current state and an action\n",
    "        \"\"\"\n",
    "        assert state in STATES, f\"Unknown state: {state}\"\n",
    "        assert action in self.get_actions(\n",
    "            state\n",
    "        ), f\"Unknown action {action} for state {state}\"\n",
    "\n",
    "        if state == \"T\":\n",
    "            raise Exception(\"The terminal state has no actions and no next state\")\n",
    "\n",
    "        if state == \"A\":\n",
    "            if action == \"right\":\n",
    "                return \"T\", 0\n",
    "            if action == \"left\":\n",
    "                return \"B\", 0\n",
    "\n",
    "        if state == \"B\":\n",
    "            return \"T\", np.random.normal(loc=-0.1)\n",
    "\n",
    "    def is_terminal(self, state):\n",
    "        assert state in STATES, f\"Unknown state: {state}\"\n",
    "        return state == \"T\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f57f98",
   "metadata": {},
   "source": [
    "Implement Double Q-learning (see page 136 in [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html)) and test it on the ```Example67MDP``` above. Notice, that the number of actions differs between the two states ($\\mathcal{A}$(```\"A\"```) $\\neq \\mathcal{A}$(```\"B\"```)), which you have to take into account in your Q-tables. See the code for ```Example67MDP``` above: you can get the set of actions available in a given state by calling ```get_actions(...)``` with the state as argument.\n",
    "\n",
    "Compare action-value estimates for ```\"left\"``` and ```\"right\"``` in state ```\"A\"```  at different times during learning when using double-Q learning and when using normal Q-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79260970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of Example 6.7 with 10 actions in B\n",
    "example = Example67MDP(10)\n",
    "\n",
    "gamma = 1\n",
    "alpha = 0.05\n",
    "\n",
    "# Create two Q-tables (feel free to use your own representation):\n",
    "Q1 = [[0 for _ in range(len(example.get_actions(state)))] for state in STATES]\n",
    "Q2 = [[0 for _ in range(len(example.get_actions(state)))] for state in STATES]\n",
    "\n",
    "# Uncomment to disable double-Q-learning:\n",
    "# Q2 = Q1\n",
    "\n",
    "# You can use the below policy method if you use the Q1 and Q2 as defined above.\n",
    "# If you have done your own representation, you probably have to modify or\n",
    "# rewrite the function below:\n",
    "\n",
    "\n",
    "def e_greedy_dql_policy(state):\n",
    "    global example\n",
    "    actions = {\n",
    "        a: epsilon / len(example.get_actions(state)) for a in example.get_actions(state)\n",
    "    }\n",
    "    # Do a Q1 + Q2 to do epsilon greedy based on both tables:\n",
    "    Q = [sum(x) for x in zip(Q1[STATES.index(state)], Q2[STATES.index(state)])]\n",
    "    actions[example.get_actions(state)[np.argmax(Q)]] = (\n",
    "        1 - epsilon + epsilon / len(example.get_actions(state))\n",
    "    )\n",
    "    return actions\n",
    "\n",
    "\n",
    "### TODO: Implement double Q-learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
