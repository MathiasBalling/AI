{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e002ba54",
      "metadata": {},
      "source": [
        "# Q-learning\n",
        "\n",
        "In this notebook, you will implement Q-learning as described in [Sutton and Barto's book, Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html). We will use the grid ```World``` class from the previous lectures."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1ff01a4",
      "metadata": {},
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 331,
      "id": "7d58d34a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /Users/balling/projects/AI/env/lib/python3.9/site-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /Users/balling/projects/AI/env/lib/python3.9/site-packages (2.2.3)\n",
            "Requirement already satisfied: tqdm in /Users/balling/projects/AI/env/lib/python3.9/site-packages (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/balling/projects/AI/env/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/balling/projects/AI/env/lib/python3.9/site-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/balling/projects/AI/env/lib/python3.9/site-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /Users/balling/projects/AI/env/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Installing collected packages: tqdm\n",
            "Successfully installed tqdm-4.67.1\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "! pip install numpy pandas tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5ab4ce0",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 332,
      "id": "0e064d24",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import sys  # We use sys to get the max value of a float\n",
        "from IPython.display import display\n",
        "import pandas as pd  # We only use pandas for displaying tables nicely\n",
        "from tqdm import tqdm\n",
        "\n",
        "pd.options.display.float_format = \"{:,.3f}\".format"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0097a47",
      "metadata": {},
      "source": [
        "### ```World``` class and globals\n",
        "\n",
        "The ```World``` is a grid represented as a two-dimensional array of characters where each character can represent free space, an obstacle, or a terminal. Each non-obstacle cell is associated with a reward that an agent gets for moving to that cell (can be 0). The size of the world is _width_ $\\times$ _height_ characters.\n",
        "\n",
        "A _state_ is a tuple $(x,y)$.\n",
        "\n",
        "An empty world is created in the ```__init__``` method. Obstacles, rewards and terminals can then be added with ```add_obstacle``` and ```add_reward```.\n",
        "\n",
        "To calculate the next state of an agent (that is, an agent is in some state $s = (x,y)$ and performs and action, $a$), ```get_next_state()```should be called."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 333,
      "id": "24bdaff8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Globals:\n",
        "ACTIONS = (\"up\", \"down\", \"left\", \"right\")\n",
        "\n",
        "# Rewards, terminals and obstacles are characters:\n",
        "REWARDS = {\" \": 0, \".\": 0.1, \"+\": 10, \"-\": -10}\n",
        "TERMINALS = (\"+\", \"-\")  # Note a terminal should also have a reward assigned\n",
        "OBSTACLES = \"#\"\n",
        "\n",
        "# Discount factor\n",
        "gamma = 1\n",
        "\n",
        "# The probability of a random move:\n",
        "rand_move_probability = 0\n",
        "\n",
        "\n",
        "class World:\n",
        "    def __init__(self, width, height):\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        # Create an empty world where the agent can move to all cells\n",
        "        self.grid = np.full((width, height), \" \", dtype=\"U1\")\n",
        "\n",
        "    def add_obstacle(self, start_x, start_y, end_x=None, end_y=None):\n",
        "        \"\"\"\n",
        "        Create an obstacle in either a single cell or rectangle.\n",
        "        \"\"\"\n",
        "        if end_x == None:\n",
        "            end_x = start_x\n",
        "        if end_y == None:\n",
        "            end_y = start_y\n",
        "\n",
        "        self.grid[start_x : end_x + 1, start_y : end_y + 1] = OBSTACLES[0]\n",
        "\n",
        "    def add_reward(self, x, y, reward):\n",
        "        assert reward in REWARDS, f\"{reward} not in {REWARDS}\"\n",
        "        self.grid[x, y] = reward\n",
        "\n",
        "    def add_terminal(self, x, y, terminal):\n",
        "        assert terminal in TERMINALS, f\"{terminal} not in {TERMINALS}\"\n",
        "        self.grid[x, y] = terminal\n",
        "\n",
        "    def is_obstacle(self, x, y):\n",
        "        if x < 0 or x >= self.width or y < 0 or y >= self.height:\n",
        "            return True\n",
        "        else:\n",
        "            return self.grid[x, y] in OBSTACLES\n",
        "\n",
        "    def is_terminal(self, x, y):\n",
        "        return self.grid[x, y] in TERMINALS\n",
        "\n",
        "    def get_reward(self, x, y):\n",
        "        \"\"\"\n",
        "        Return the reward associated with a given location\n",
        "        \"\"\"\n",
        "        return REWARDS[self.grid[x, y]]\n",
        "\n",
        "    def get_next_state(self, current_state, action):\n",
        "        \"\"\"\n",
        "        Get the next state given a current state and an action. The outcome can be\n",
        "        stochastic  where rand_move_probability determines the probability of\n",
        "        ignoring the action and performing a random move.\n",
        "        \"\"\"\n",
        "        assert action in ACTIONS, f\"Unknown acion {action} must be one of {ACTIONS}\"\n",
        "\n",
        "        x, y = current_state\n",
        "\n",
        "        # If our current state is a terminal, there is no next state\n",
        "        if self.grid[x, y] in TERMINALS:\n",
        "            return None\n",
        "\n",
        "        # Check of a random action should be performed:\n",
        "        if np.random.rand() < rand_move_probability:\n",
        "            action = np.random.choice(ACTIONS)\n",
        "\n",
        "        if action == \"up\":\n",
        "            y -= 1\n",
        "        elif action == \"down\":\n",
        "            y += 1\n",
        "        elif action == \"left\":\n",
        "            x -= 1\n",
        "        elif action == \"right\":\n",
        "            x += 1\n",
        "\n",
        "        # If the next state is an obstacle, stay in the current state\n",
        "        return (x, y) if not self.is_obstacle(x, y) else current_state"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a60c7ddb",
      "metadata": {},
      "source": [
        "## A simple world"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 334,
      "id": "7da198ad",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' ' ']\n",
            " [' ' ' ' ' ' '+']]\n"
          ]
        }
      ],
      "source": [
        "world = World(4, 6)\n",
        "\n",
        "# Since we only focus on episodic tasks, we must have a terminal state that the\n",
        "# agent eventually reaches\n",
        "world.add_terminal(3, 5, \"+\")\n",
        "\n",
        "print(world.grid.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "822d3a56",
      "metadata": {},
      "source": [
        "## Exercise: Q-learning\n",
        "\n",
        "Implement and test Q-learning. You should be able to base much of your code on your implementation of SARSA. Since Q-learning is an off-policy method, we can use whatever behavior policy we want during training, but the choice of behavioral policy still manners so it is a good idea to balance exploration and exploitation. During testing, we can then use the learnt policy (the target policy).\n",
        "\n",
        "As for the behavior policy, you can use an simple $\\epsilon$-greedy policy, but you can also experiment with alternatives, for instance, optimistic initial values.\n",
        "\n",
        "See page 131 in [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html) for the Q-learning algorithm.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 335,
      "id": "9311557e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def greedy_policy(Q, state, epsilon):\n",
        "    a = {action: epsilon / len(ACTIONS) for action in ACTIONS}\n",
        "\n",
        "    _, max_value = max(Q[state].items(), key=lambda x: x[1])\n",
        "    # Count cells that have the same max value\n",
        "    max_value_count = sum(Q[state][action] == max_value for action in ACTIONS)\n",
        "    for action in ACTIONS:\n",
        "        if Q[state][action] == max_value:\n",
        "            a[action] += (1 - epsilon) / max_value_count\n",
        "\n",
        "    return a\n",
        "\n",
        "\n",
        "def q_learning(world: World, start_state, policy, Q, gamma=0.9, alpha=0.1, epsilon=0.1):\n",
        "    current_state = start_state\n",
        "    while not world.is_terminal(*current_state):\n",
        "        # Choose the next action based on the epsilon-greedy policy\n",
        "        possible_actions = policy(Q, current_state, epsilon)\n",
        "\n",
        "        # Initialize the starting action\n",
        "        current_action = random.choices(\n",
        "            population=list(possible_actions.keys()),\n",
        "            weights=list(possible_actions.values()),\n",
        "            k=1,\n",
        "        )[0]\n",
        "\n",
        "        # Get the next state and reward\n",
        "        next_state = world.get_next_state(current_state, current_action)\n",
        "        reward = world.get_reward(*next_state)\n",
        "\n",
        "        # Find the best next action in the next state\n",
        "        best_next_action_value = max(Q[next_state].values())\n",
        "\n",
        "        # Update the Q-table\n",
        "        Q[current_state][current_action] += alpha * (\n",
        "            reward + gamma * best_next_action_value - Q[current_state][current_action]\n",
        "        )\n",
        "\n",
        "        # Update the state\n",
        "        current_state = next_state\n",
        "\n",
        "    # Return the Q-table after training to be used as a policy\n",
        "    return Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 336,
      "id": "c3a0cdb1",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td></td>\n",
              "      <td>+</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0  1  2\n",
              "0         \n",
              "1         \n",
              "2     +   "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>down</td>\n",
              "      <td>left</td>\n",
              "      <td>down</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>down</td>\n",
              "      <td>down</td>\n",
              "      <td>down</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>right</td>\n",
              "      <td>termnal</td>\n",
              "      <td>left</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       0        1     2\n",
              "0   down     left  down\n",
              "1   down     down  down\n",
              "2  right  termnal  left"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "ACTIONS = (\"up\", \"down\", \"left\", \"right\")\n",
        "world = World(3, 3)\n",
        "world.add_terminal(1, 2, \"+\")\n",
        "display(pd.DataFrame(world.grid.T))\n",
        "\n",
        "Q = {}\n",
        "for x in range(world.width):\n",
        "    for y in range(world.height):\n",
        "        Q[(x, y)] = {action: 0.0 for action in ACTIONS}\n",
        "\n",
        "nb_episodes = 10000\n",
        "for i in range(nb_episodes):\n",
        "    Q = q_learning(\n",
        "        world=world,\n",
        "        start_state=(0, 0),\n",
        "        policy=greedy_policy,\n",
        "        Q=Q,\n",
        "        gamma=0.9,\n",
        "        alpha=0.1,\n",
        "        epsilon=0.1,\n",
        "    )\n",
        "\n",
        "final_policy = np.full((world.width, world.height), \"          \")\n",
        "for i in range(world.width):\n",
        "    for j in range(world.height):\n",
        "        if world.is_terminal(i, j):\n",
        "            final_policy[(i, j)] = \"termnal\"\n",
        "        elif world.is_obstacle(i, j):\n",
        "            final_policy[(i, j)] = \"#\"\n",
        "        else:\n",
        "            final_policy[(i, j)] = max(Q[(i, j)], key=Q[(i, j)].get)\n",
        "display(pd.DataFrame(final_policy.T))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e151bbd",
      "metadata": {},
      "source": [
        "## Exercise: Compare Q-learning and SARSA\n",
        "\n",
        "Setup experiments to compare the performance of Q-learning and SARSA. You can use different ```Worlds``` and test different parameter setting, e.g. for $\\alpha$ and $\\epsilon$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 337,
      "id": "e28d13a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "### TODO: Implement your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b12733ca",
      "metadata": {},
      "source": [
        "## Optional exercise: Maximization Bias and Double Learning\n",
        "\n",
        "Below is an implementation of the task shown in Example 6.7 on page 134 in [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html). There are two states, ```A``` and ```B``` where the agent can perform actions, and a terminal state ```T```. ```A``` and ```B``` have different actions available:\n",
        "\n",
        "* ```A``` has ```left``` (to ```B```) and ```right``` to the terminal state\n",
        "* ```B``` has a larger number of actions all leading to a terminal state.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 338,
      "id": "67552b6b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# States \"A\" and \"B\" have actions while \"T\" is a terminal state.\n",
        "STATES = (\"A\", \"B\", \"T\")\n",
        "\n",
        "\n",
        "class Example67MDP:\n",
        "    def __init__(self, number_of_B_actions):\n",
        "        \"\"\"\n",
        "        Create an example and set the number of outgoing actions for state \"B\"\n",
        "        (in the book, they do not give a specific number, but merely write that\n",
        "        from \"B\" there \"are many possible actions all of which cause immediate\n",
        "        termination with a reward drawn from a normal distribution with mean\n",
        "        -0.1 and variance 1. So, feel free to play with different number of\n",
        "        actions in state B)\n",
        "\n",
        "        \"\"\"\n",
        "        self.number_of_B_actions = number_of_B_actions\n",
        "\n",
        "    def get_actions(self, state):\n",
        "        \"\"\"\n",
        "        Returns the set of actions availabe in a given state (a tuple\n",
        "        with strings).\n",
        "        \"\"\"\n",
        "        assert state in STATES, f\"State must be one of {STATES}, not {state}\"\n",
        "        if state == \"A\":\n",
        "            return (\"left\", \"right\")\n",
        "        if state == \"B\":\n",
        "            return tuple(f\"{i}\" for i in range(self.number_of_B_actions))\n",
        "        if state == \"T\":\n",
        "            return tuple(\"N\")\n",
        "\n",
        "    def get_next_state_and_reward(self, state, action):\n",
        "        \"\"\"\n",
        "        Get the next state and reward given a current state and an action\n",
        "        \"\"\"\n",
        "        assert state in STATES, f\"Unknown state: {state}\"\n",
        "        assert action in self.get_actions(\n",
        "            state\n",
        "        ), f\"Unknown action {action} for state {state}\"\n",
        "\n",
        "        if state == \"T\":\n",
        "            raise Exception(\"The terminal state has no actions and no next state\")\n",
        "\n",
        "        if state == \"A\":\n",
        "            if action == \"right\":\n",
        "                return \"T\", 0\n",
        "            if action == \"left\":\n",
        "                return \"B\", 0\n",
        "\n",
        "        if state == \"B\":\n",
        "            return \"T\", np.random.normal(loc=-0.1)\n",
        "\n",
        "    def is_terminal(self, state):\n",
        "        assert state in STATES, f\"Unknown state: {state}\"\n",
        "        return state == \"T\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25f57f98",
      "metadata": {},
      "source": [
        "Implement Double Q-learning (see page 136 in [Introduction to Reinforcement Learning](http://incompleteideas.net/book/the-book-2nd.html)) and test it on the ```Example67MDP``` above. Notice, that the number of actions differs between the two states ($\\mathcal{A}$(```\"A\"```) $\\neq \\mathcal{A}$(```\"B\"```)), which you have to take into account in your Q-tables. See the code for ```Example67MDP``` above: you can get the set of actions available in a given state by calling ```get_actions(...)``` with the state as argument.\n",
        "\n",
        "Compare action-value estimates for ```\"left\"``` and ```\"right\"``` in state ```\"A\"```  at different times during learning when using double-Q learning and when using normal Q-learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 339,
      "id": "79260970",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an instance of Example 6.7 with 10 actions in B\n",
        "# example = Example67MDP(10)\n",
        "#\n",
        "# gamma = 1\n",
        "# alpha = 0.05\n",
        "#\n",
        "# # Create two Q-tables (feel free to use your own representation):\n",
        "# Q1 = [[0 for _ in range(len(example.get_actions(state)))] for state in STATES]\n",
        "# Q2 = [[0 for _ in range(len(example.get_actions(state)))] for state in STATES]\n",
        "\n",
        "# Uncomment to disable double-Q-learning:\n",
        "# Q2 = Q1\n",
        "\n",
        "# You can use the below policy method if you use the Q1 and Q2 as defined above.\n",
        "# If you have done your own representation, you probably have to modify or\n",
        "# rewrite the function below:\n",
        "\n",
        "\n",
        "# def e_greedy_dql_policy(state):\n",
        "#     global example\n",
        "#     actions = {\n",
        "#         a: epsilon / len(example.get_actions(state)) for a in example.get_actions(state)\n",
        "#     }\n",
        "#     # Do a Q1 + Q2 to do epsilon greedy based on both tables:\n",
        "#     Q = [sum(x) for x in zip(Q1[STATES.index(state)], Q2[STATES.index(state)])]\n",
        "#     actions[example.get_actions(state)[np.argmax(Q)]] = (\n",
        "#         1 - epsilon + epsilon / len(example.get_actions(state))\n",
        "#     )\n",
        "#     return actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 340,
      "id": "1a8f0bc1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def double_q_learning(\n",
        "    world: World, start_state, policy, Q1, Q2, gamma=0.9, alpha=0.1, epsilon=0.1\n",
        "):\n",
        "    current_state = start_state\n",
        "    while not world.is_terminal(*current_state):\n",
        "        # Choose the next action based on the epsilon-greedy policy\n",
        "        possible_actions = policy(Q1, Q2, current_state, epsilon)\n",
        "\n",
        "        # Initialize the starting action\n",
        "        current_action = random.choices(\n",
        "            population=list(possible_actions.keys()),\n",
        "            weights=list(possible_actions.values()),\n",
        "            k=1,\n",
        "        )[0]\n",
        "\n",
        "        # Get the next state and reward\n",
        "        next_state = world.get_next_state(current_state, current_action)\n",
        "        reward = world.get_reward(*next_state)\n",
        "\n",
        "        if np.random.rand() < 0.5:\n",
        "            # Find best action from next state using Q1\n",
        "            q1_best_action = max(Q1[next_state], key=Q1[next_state].get)\n",
        "            # Update the Q1-table\n",
        "            Q1[current_state][current_action] += alpha * (\n",
        "                reward\n",
        "                + gamma * Q2[next_state][q1_best_action]\n",
        "                - Q1[current_state][current_action]\n",
        "            )\n",
        "        else:\n",
        "            # Find best action from next state using Q1\n",
        "            q2_best_action = max(Q2[next_state], key=Q2[next_state].get)\n",
        "            # Update the Q2-table\n",
        "            Q2[current_state][current_action] += alpha * (\n",
        "                reward\n",
        "                + gamma * Q1[next_state][q2_best_action]\n",
        "                - Q2[current_state][current_action]\n",
        "            )\n",
        "\n",
        "        # Update the state\n",
        "        current_state = next_state\n",
        "\n",
        "    # Return the Q-table after training to be used as a policy\n",
        "    return Q1, Q2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 341,
      "id": "56360e54",
      "metadata": {},
      "outputs": [],
      "source": [
        "def greedy_policy_double(Q1, Q2, state, epsilon):\n",
        "    a = {action: epsilon / len(ACTIONS) for action in ACTIONS}\n",
        "\n",
        "    # Combine Q1 and Q2 values\n",
        "    combined_Q = {action: Q1[state][action] + Q2[state][action] for action in ACTIONS}\n",
        "\n",
        "    _, max_value = max(combined_Q.items(), key=lambda x: x[1])\n",
        "    # Count cells that have the same max value\n",
        "    max_value_count = sum(combined_Q[action] == max_value for action in ACTIONS)\n",
        "    for action in ACTIONS:\n",
        "        if combined_Q[action] == max_value:\n",
        "            a[action] += (1 - epsilon) / max_value_count\n",
        "\n",
        "    return a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 343,
      "id": "95590ee0",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>+</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0  1  2  3  4  5\n",
              "0                  \n",
              "1                  \n",
              "2                  \n",
              "3                  \n",
              "4                  \n",
              "5                 +"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████████████████████████████████████████████████████████████| 50000/50000 [00:03<00:00, 14205.82it/s]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>down</td>\n",
              "      <td>down</td>\n",
              "      <td>down</td>\n",
              "      <td>down</td>\n",
              "      <td>right</td>\n",
              "      <td>down</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>right</td>\n",
              "      <td>right</td>\n",
              "      <td>right</td>\n",
              "      <td>down</td>\n",
              "      <td>down</td>\n",
              "      <td>down</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>up</td>\n",
              "      <td>up</td>\n",
              "      <td>up</td>\n",
              "      <td>right</td>\n",
              "      <td>right</td>\n",
              "      <td>down</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>right</td>\n",
              "      <td>up</td>\n",
              "      <td>up</td>\n",
              "      <td>up</td>\n",
              "      <td>right</td>\n",
              "      <td>down</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>right</td>\n",
              "      <td>up</td>\n",
              "      <td>down</td>\n",
              "      <td>down</td>\n",
              "      <td>right</td>\n",
              "      <td>down</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>right</td>\n",
              "      <td>right</td>\n",
              "      <td>right</td>\n",
              "      <td>right</td>\n",
              "      <td>right</td>\n",
              "      <td>termnal</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       0      1      2      3      4        5\n",
              "0   down   down   down   down  right     down\n",
              "1  right  right  right   down   down     down\n",
              "2     up     up     up  right  right     down\n",
              "3  right     up     up     up  right     down\n",
              "4  right     up   down   down  right     down\n",
              "5  right  right  right  right  right  termnal"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "ACTIONS = (\"up\", \"down\", \"left\", \"right\")\n",
        "world = World(6, 6)\n",
        "world.add_terminal(5, 5, \"+\")\n",
        "display(pd.DataFrame(world.grid.T))\n",
        "\n",
        "Q1, Q2 = {}, {}\n",
        "for x in range(world.width):\n",
        "    for y in range(world.height):\n",
        "        Q1[(x, y)] = {action: 0.0 for action in ACTIONS}\n",
        "        Q2[(x, y)] = {action: 0.0 for action in ACTIONS}\n",
        "\n",
        "nb_episodes = 50000\n",
        "for i in tqdm(range(nb_episodes)):\n",
        "    Q1, Q2 = double_q_learning(\n",
        "        world=world,\n",
        "        start_state=(0, 0),\n",
        "        policy=greedy_policy_double,\n",
        "        Q1=Q1,\n",
        "        Q2=Q2,\n",
        "        gamma=1,\n",
        "        alpha=0.05,\n",
        "        epsilon=0.15,\n",
        "    )\n",
        "\n",
        "final_policy = np.full((world.width, world.height), \"          \")\n",
        "for i in range(world.width):\n",
        "    for j in range(world.height):\n",
        "        if world.is_terminal(i, j):\n",
        "            final_policy[(i, j)] = \"termnal\"\n",
        "        elif world.is_obstacle(i, j):\n",
        "            final_policy[(i, j)] = \"#\"\n",
        "        else:\n",
        "            combined_Q = {\n",
        "                action: Q1[(i, j)][action] + Q2[(i, j)][action] for action in ACTIONS\n",
        "            }\n",
        "            final_policy[(i, j)] = max(combined_Q, key=combined_Q.get)\n",
        "\n",
        "display(pd.DataFrame(final_policy.T))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
